{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc189d2-656e-41d4-be84-dcf6ccf9537b",
   "metadata": {},
   "source": [
    "# Representación semántica y búsqueda\n",
    "Autor: Eric S. Tellez <eric.tellez@infotec.mx>\n",
    "\n",
    "Cuando la información esta poco específicada, las palabras adecuadas podrían ser dificiles de tener o limitar. En estos casos, la representaciones semánticas que permiten buscar _lo que se desea_ por medio de conceptos nos acerca más a la posibilidad de obtener información útil.\n",
    "\n",
    "Como anteriormente se presento, una de las representaciones semánticas más aceptadas son aquellas basadas en word embeddings. Recordando, estas son representaciones vectoriales de cada palabra, donde la semántica se asocia con la estructura en un espacio métrico, i.e., cercano en la métrica significa similar semánticamente.\n",
    "\n",
    "# Representación semántica de documentos basada en _word embeddings_\n",
    "Un documento puede verse como una bolsa de palabras, como en el modelado tradicional, pero en lugar de usar los términos como símbolos se pueden usar los vectores semánticos. El como usar esta nube de puntos multidimensional para obtener una representación computacionalmente manejable y a la vez eficaz, es un tema que ha llevado al desarrollo de modelos cada vez más complejos, como pueden ser los grandes modelos de lenguaje. \n",
    "\n",
    "La manera forma más directa de definir una representación semántica de documentos es el uso de centroides, esto es, un vector de la misma dimensión que sumariza a un conjunto de vectores usando su media geométrica. De manera más precisa, sea $E$ una matrix $m \\times n$ de embeddings del vocabulario, i.e., considerando un vocabulario de tamaño $n$ y dimensión $m$.\n",
    "Sea $D_{m,\\ell}$ la submatriz de $E$ que contiene los $\\ell$ vectores del documento $\\textsf{doc}$. El prototipo $\\vec{d}$ de $D$ esta definido de la siguiente forma:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{d} = \\frac{1}{\\ell} \\sum_{i=1}^\\ell D_i\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Donde $D_i$ es el i-ésimo vector columna de $D$. Dado que se usa el coseno como similitud, también es factible el cálculo como la suma vectorial normalizada, esto es,\n",
    "\n",
    "$$ \\vec{d} = \\frac{\\sum_{i=1}^\\ell D_i}{\\lVert \\sum_{i=1}^\\ell D_i \\rVert}$$\n",
    "\n",
    "También es posible añadir información local basada en la representación de bolsa de plabras. Por ejemplo, realizar una suma pesada usando el peso de las palabras mediante la frecuencia normalizada de término (TF) o la probabilidad de término $pt$, ver unidad 3 para más información.\n",
    "\n",
    "$$\\vec{d} = \\sum_{i=1}^{\\ell} \\textsf{TF}(t_i, \\textsf{doc}) \\cdot D_i$$\n",
    "\n",
    "Donde $t_i$ es el $i$-ésimo término de $\\textsf{doc}$. Note que el orden no se captura y solo es necesario para la notación.\n",
    "\n",
    "## Usando la nube de puntos\n",
    "\n",
    "La representación de nube de puntos puede usarse directamente sin realizar prototipos, sin embargo, es posible que dada su complejidad, dimensión intrinseca, tenga poca utilidad en grandes volumenes de información. Por ejemplo, es posible utilizar la distancia de [Hausdorff](https://en.wikipedia.org/wiki/Hausdorff_distance); que entre dos nubes de puntos (documentos para este caso) esta definida como sigue:\n",
    "\n",
    "$$ H_d(U, V) = \\max \\left\\{ \\max_{u \\in U} \\min_{x \\in V} d(u, x), \\max_{v \\in V} \\min_{x \\in U} d(v, x) \\right\\} $$\n",
    "\n",
    "Donde $\\textsf{nn}$ encuentra el vecino cercano del primer argumento en el segundo. Podemos ver la intuición de esta distancia, analizando las partes de la expresión. \n",
    "La diferencia entre ambos conjuntos se representa con el máximo de las distancias mínimas. Note que esta parte es idéntica a resolver una búsqueda de vecinos cercanos. La parte más externa de la expresión se repite para los dos conjuntos para preservar la simétria de la función.\n",
    "\n",
    "De la misma forma, es posible obtener variaciones de interés como considerar todas las distancias cercanas en lugar de solo las máximas, lo cual puede reducir formas caprichosas de _outlayers_.\n",
    "\n",
    "$$ H^+_d(U, V) = \\max \\left\\{ \\sum_{u \\in U} \\min_{x \\in V} d(u, x), \\sum_{v \\in V} \\min_{x \\in U} d(v, x) \\right\\}$$\n",
    "\n",
    "La información local de una representación de bolsa de palabras también puede ser aprovechada, por ejemplo, añadiendo información de $\\textsf{TF}$\n",
    "\n",
    "$$ H^\\textsf{TF}_d(U, V) = \\max \\left\\{ \\sum_{u \\in U} {\\textsf{TF}(u, U) \\min_{x \\in V} d(u, x)}, \\sum_{v \\in V} {\\textsf{TF}(v, V) \\min_{x \\in U} d(v, x)} \\right\\} $$\n",
    "\n",
    "# Notas adicionales\n",
    "La versión de $H_d$ que usa la probabilidad de término $\\textsf{pt}$ es similar a la aproximación 2 listada en [@KSKW2015]. Los prototipos son similares a la aproximación 1 del mismo artículo. En el artículo se presenta la función de distancia _word mover's distance_ (WMD) que es una adecuación de la Earths mover's distance (EMD) [PW2009]. EMD es la solución óptima a un problema de transportación, la WMD es la adaptación teniendo en cuenta embeddings de palabras.\n",
    "\n",
    "# Ejemplo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd04785e-aa4e-493c-9aa8-5c54c8e1f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/IR-2022/Unidades`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "!isfile(\"Manifest.toml\") && Pkg.add([\n",
    "    PackageSpec(name=\"SimilaritySearch\", version=\"0.9\"),\n",
    "    PackageSpec(name=\"TextSearch\", version=\"0.12\"),\n",
    "    PackageSpec(name=\"Plots\"),\n",
    "    PackageSpec(name=\"KNearestCenters\"),\n",
    "    PackageSpec(name=\"HypertextLiteral\"),\n",
    "    PackageSpec(name=\"CSV\"),\n",
    "    PackageSpec(name=\"DataFrames\"),\n",
    "    PackageSpec(name=\"Embeddings\", version=\"0.4\")\n",
    "])\n",
    "\n",
    "using SimilaritySearch, TextSearch, Plots, KNearestCenters, LinearAlgebra, Embeddings, HypertextLiteral, CSV, DataFrames, Base64, Random\n",
    "using Downloads: download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9f7f49-5c32-41ea-95f5-b8f1670c2842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m      Status\u001b[22m\u001b[39m `~/IR-2022/Unidades/Project.toml`\n",
      " \u001b[90m [944b1d66] \u001b[39mCodecZlib v0.7.0\n",
      " \u001b[90m [ac1192a8] \u001b[39mHypertextLiteral v0.9.4\n",
      " \u001b[90m [b20bd276] \u001b[39mInvertedFiles v0.4.1 `../../Research/InvertedFiles.jl`\n",
      " \u001b[90m [682c06a0] \u001b[39mJSON v0.21.3\n",
      " \u001b[90m [4dca28ae] \u001b[39mKNearestCenters v0.7.1\n",
      " \u001b[90m [8ef0a80b] \u001b[39mLanguages v0.4.3\n",
      " \u001b[90m [eb30cadb] \u001b[39mMLDatasets v0.7.3\n",
      " \u001b[90m [053f045d] \u001b[39mSimilaritySearch v0.9.4 `../../Research/SimilaritySearch.jl`\n",
      " \u001b[90m [fb8f903a] \u001b[39mSnowball v0.1.0\n",
      " \u001b[90m [2913bbd2] \u001b[39mStatsBase v0.33.18\n",
      " \u001b[90m [7f6f6c8a] \u001b[39mTextSearch v0.12.5 `../../Research/TextSearch.jl`\n"
     ]
    }
   ],
   "source": [
    "]status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d9bcb8-60f5-42d5-adb3-dca1f1b480b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scores (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function knn(index, labels, q, k)\n",
    "    res = KnnResult(k)\n",
    "    search(index, q, res)\n",
    "    mode(labels[idview(res)])\n",
    "end\n",
    "\n",
    "function mymode(c, labels, f)\n",
    "    n = length(c)\n",
    "    empty!(f)\n",
    "    for id in c\n",
    "        id == 0 && break  # searchbatch stores zeros at the end of the result when the result set is smaller than the required one\n",
    "        l = labels[id]\n",
    "        f[l] = get(f, l, 0) + 1\n",
    "    end\n",
    "    \n",
    "    if length(f) == 0\n",
    "        rand(labels)\n",
    "    else\n",
    "        argmax(last, f) |> first\n",
    "    end\n",
    "end\n",
    "\n",
    "function knn(I, labels)\n",
    "    f = Dict{eltype(labels), Int}()\n",
    "    [mymode(c, labels, f) for c in eachcol(I)]\n",
    "end\n",
    "\n",
    "function scores(gold, pred)\n",
    "    s = classification_scores(gold, pred)\n",
    "    (macrof1=s.macrof1, macrorecall=s.macrorecall, accuracy=s.accuracy)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4fcb23f-9564-4910-9a4f-1f5d4a582d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_model_and_vectors (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function text_model_and_vectors(\n",
    "        corpus, vocab;\n",
    "        localweighting=TpWeighting(),\n",
    "        globalweighting=BinaryGlobalWeighting(),\n",
    "        nlist=[1],\n",
    "        qlist=[],\n",
    "        slist=[],\n",
    "        group_usr=true,\n",
    "        group_url=true,\n",
    "        group_num=true,\n",
    "        del_diac=true,\n",
    "        lc=true\n",
    "    )\n",
    "\n",
    "    voc = Vocabulary(length(corpus))\n",
    "    for v in vocab\n",
    "       push!(voc, v)\n",
    "    end\n",
    "    \n",
    "    textconfig = TextConfig(; group_usr, group_url, del_diac, lc, group_num, nlist, qlist, slist)\n",
    "    tokenize_and_append!(voc, textconfig, corpus)\n",
    "    model = VectorModel(globalweighting, localweighting, voc)\n",
    "    (; textconfig, model, voc)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8bad4e-c31b-4506-8341-47c4130d664a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_index (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function embeddings()\n",
    "    embfile = \"../data/MX.vec\"\n",
    "    !isfile(embfile) && download(\"http://geo.ingeotec.mx/~sadit/regional-spanish-models/MX.vec\", embfile)\n",
    "    emb = load_embeddings(FastText_Text, embfile)  # you can change with any of the available embeddings in `Embeddings`\n",
    "    for c in eachcol(emb.embeddings)\n",
    "        normalize!(c)\n",
    "    end\n",
    "    \n",
    "    (; X=emb.embeddings, vocab=emb.vocab)\n",
    "end\n",
    "\n",
    "function vectorize_as_prototype!(c, text, E, T)\n",
    "    x = vectorize(T.model, T.textconfig, text; normalize=false)\n",
    "    if length(x) == 1 && haskey(x, 0)\n",
    "        #@warn \"empty vector $(Int(i)) selecting a random vector for it \" # $(corpus[i])\n",
    "        rand!(c)\n",
    "    else\n",
    "        for (id, weight) in x\n",
    "            id == 0 && continue\n",
    "             c .= c .+ weight .* view(E.X, :, id)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    normalize!(c)\n",
    "end\n",
    "\n",
    "function vectorize_corpus_as_prototypes(corpus, E, T)\n",
    "    dim = size(E.X, 1)\n",
    "    n = length(corpus)\n",
    "    C = zeros(Float32, dim, n)\n",
    "    # Threads.@threads\n",
    "    for i in 1:n\n",
    "        vectorize_as_prototype!(view(C, :, i), corpus[i], E, T)\n",
    "    end\n",
    "    \n",
    "    C\n",
    "end\n",
    "\n",
    "function create_index(db)\n",
    "    dist = NormalizedCosineDistance()\n",
    "    index = SearchGraph(; dist, db, verbose=false)\n",
    "    index!(index; callbacks=SearchGraphCallbacks(MinRecall(0.9)))\n",
    "    optimize!(index, MinRecall(0.9))\n",
    "    index\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d1dc14b-02cb-45ab-8466-5783251ef877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(textconfig = TextConfig(true, false, false, true, true, true, false, true, Int8[], Int8[1], Skipgram[], IdentityTokenTransformation()), model = {VectorModel global_weighting=BinaryGlobalWeighting(), local_weighting=TpWeighting(), train-voc=447763, train-n=41998, maxoccs=59524}, voc = Vocabulary([\"</s>\", \"_usr\", \"que\", \"de\", \",\", \".\", \"y\", \"a\", \"la\", \"no\"  …  \"#alejandromoreno\", \"tantitititiititiitititititititititititititititititititititititita\", \"multiejecucion\", \"#marcoscastellanos\", \"devolviendola\", \"senadoz\", \"#alitomoreno\", \"dastis\", \"javerianizacion\", \"#bundesliga\"], Int32[0, 25844, 20315, 59524, 31605, 21433, 17403, 20211, 34402, 6002  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1], Int32[0, 25844, 20315, 59524, 31605, 21433, 17403, 20211, 34402, 6002  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Dict{String, UInt32}(\"alaann\" => 0x0005f201, \"coprolito\" => 0x0005f0e5, \"alissonrojas\" => 0x00018ff1, \"copeamos\" => 0x0004490d, \"inhiben\" => 0x00015522, \"btardes\" => 0x00028895, \"guardalo\" => 0x00004561, \"kokis\" => 0x00030576, \"#janiotpm\" => 0x0006bf24, \"sentando\" => 0x000087bd…), 41998))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = embeddings()\n",
    "D = CSV.read(\"../data/spanish-news-in-twitter.csv\", DataFrame; delim=',')\n",
    "D[:, :text] = String.(base64decode.(D.text))\n",
    "dropmissing!(D)\n",
    "T = text_model_and_vectors(D.text, E.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "593497b1-0da0-45e3-83ae-54507b233311",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access 300×438136 Matrix{Float32} at index [1:300, 438137]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 300×438136 Matrix{Float32} at index [1:300, 438137]",
      "",
      "Stacktrace:",
      " [1] throw_boundserror(A::Matrix{Float32}, I::Tuple{Base.Slice{Base.OneTo{Int64}}, Int64})",
      "   @ Base ./abstractarray.jl:691",
      " [2] checkbounds",
      "   @ ./abstractarray.jl:656 [inlined]",
      " [3] view",
      "   @ ./subarray.jl:177 [inlined]",
      " [4] vectorize_as_prototype!(c::SubArray{Float32, 1, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}, text::String, E::NamedTuple{(:X, :vocab), Tuple{Matrix{Float32}, Vector{String}}}, T::NamedTuple{(:textconfig, :model, :voc), Tuple{TextConfig, VectorModel{BinaryGlobalWeighting, TpWeighting}, Vocabulary}})",
      "   @ Main ./In[5]:20",
      " [5] vectorize_corpus_as_prototypes(corpus::SentinelArrays.ChainedVector{String, Vector{String}}, E::NamedTuple{(:X, :vocab), Tuple{Matrix{Float32}, Vector{String}}}, T::NamedTuple{(:textconfig, :model, :voc), Tuple{TextConfig, VectorModel{BinaryGlobalWeighting, TpWeighting}, Vocabulary}})",
      "   @ Main ./In[5]:33",
      " [6] top-level scope",
      "   @ ./timing.jl:220 [inlined]",
      " [7] top-level scope",
      "   @ ./In[7]:0",
      " [8] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [9] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "@time C = vectorize_corpus_as_prototypes(D.text, E, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff6403-f5e7-470e-90e0-baa79eccb793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bcb60ff-68c0-4c09-a8dd-70079ba59994",
   "metadata": {},
   "source": [
    "### Se crea el índice métrico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d9108c0-02a7-4afe-bfa2-7b436f7bd0f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: C not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: C not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./timing.jl:220 [inlined]",
      " [2] top-level scope",
      "   @ ./In[8]:0",
      " [3] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "@time index = create_index(MatrixDatabase(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e629f24-ecf3-4908-a100-b70559dca89a",
   "metadata": {},
   "source": [
    "### Búsqueda de todos los vecinos cercanos en el vocabulario, observe la conveniencia del uso de un índice\n",
    "\n",
    "Entre más sean las consultas más se ve la bondad (~42k consultas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43463a8-0c73-4903-a78c-6b2405ddf812",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: index not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: index not defined",
      "",
      "Stacktrace:",
      " [1] macro expansion",
      "   @ ./timing.jl:299 [inlined]",
      " [2] top-level scope",
      "   @ ./In[9]:2",
      " [3] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "let k = 32\n",
    "    t1 = @elapsed I, _ = allknn(index, k)\n",
    "    # WARNING: Don't run the following line, it takes too much time\n",
    "    t2 = @elapsed gI, _ = allknn(ExhaustiveSearch(; db=index.db, dist=index.dist), k)\n",
    "    r = macrorecall(gI, I)\n",
    "    n = size(I, 2)\n",
    "    labels = String.(D.screen_name)\n",
    "    s1 = scores(labels, knn(I, labels))\n",
    "    s2 = scores(labels, knn(gI, labels))\n",
    "    \n",
    "    @htl \"\"\"\n",
    "    <div>macro-recall: $r, n: $n</div>\n",
    "    <div>searchgraph search time: $t1, scores: $s1</div>\n",
    "    <div>brute force search time: $t2, scores: $s2</div>\n",
    "    \"\"\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a41c3f-e904-46a4-99a5-0c3d61fa6722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51b68d72-5256-49ad-b22b-e896f58bc29f",
   "metadata": {},
   "source": [
    "### Búsqueda y presentación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e045135e-4189-47cc-9513-72a5000527f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "search_and_display (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function search_and_display(index, qtext, k, D, E, T)\n",
    "    res = KnnResult(k)\n",
    "    q = zeros(Float32, size(E.X, 1))\n",
    "    vectorize_as_prototype!(q, qtext, E, T)\n",
    "    @time search(index, q, res)\n",
    "    \n",
    "    L = []\n",
    "    for (j, (id, d)) in enumerate(res)\n",
    "        push!(L, @htl \"<tr><td>$j</td><td>$id</td><td>$(round(d, digits=3))</td> <td>$(D.screen_name[id])</td><td> $(D.text[id])</td> </tr>\")\n",
    "    end\n",
    "\n",
    "    display(@htl \"\"\"<h2>resultados for \"$qtext\"</h2>\n",
    "    <table>\n",
    "    <th>  <td>id</td> <td>dist</td> <td>user</td> </td>message<td> </th>\n",
    "        $L\n",
    "    </table>\n",
    "    \"\"\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1123924-05b2-4799-b2f7-18b3d2c46f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Ejemplos de búsqueda</h1>"
      ],
      "text/plain": [
       "<h1>Ejemplos de búsqueda</h1>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: index not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: index not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[11]:3",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "\n",
    "display(@htl \"<h1>Ejemplos de búsqueda</h1>\")\n",
    "search_and_display(index, \"el gobierno de andres manuel lopez\", 7, D, E, T)\n",
    "search_and_display(index, \"trafico de drogas\", 7, D, E, T)\n",
    "search_and_display(index, \"covid corona virus\", 7, D, E, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4028733-5dd7-46e0-addb-5f7959766695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Ejemplos de búsqueda (mensajes aleatorios)</h1>"
      ],
      "text/plain": [
       "<h1>Ejemplos de búsqueda (mensajes aleatorios)</h1>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: index not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: index not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[12]:5",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "\n",
    "display(@htl \"<h1>Ejemplos de búsqueda (mensajes aleatorios)</h1>\")\n",
    "for i in 1:3\n",
    "    for qid in rand(1:length(D.text))\n",
    "        search_and_display(index, D.text[qid], 7, D, E, T)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f09a9-75aa-49c5-9e35-4c486defa289",
   "metadata": {},
   "source": [
    "# Actividades\n",
    "- Reproduzca el ejercicio de este notebook, use embeddings para español, cambié los ejemplos. Se sugiere el uso de <https://ingeotec.github.io/regional-spanish-models/> donde encontrará modelos fastText regionalizados del español, pero puede usar otros embeddings.\n",
    "- ¿Qué piensa de las diferencias de tamaño entre los documentos y las consultas? esto como afecta a la representación semántica.\n",
    "- ¿Cuál sería el símil de bigramas y trigramas para este esquema de representación semántica? Implementelo.\n",
    "- Defina $H^{pt}_d$, donde $pt$ es la probabilidad de ocurrencia de término.- Defina $H^{pt}_d$, donde $pt$ es la probabilidad de ocurrencia de término.\n",
    "- Implemente $H^\\textsf{TF}_d$ y $H^\\textsf{pt}_d$ de manera secuencial y con un índice métrico. Si usa Julia considere `SimilaritySearch.jl` y si usa Python considere `faiss`.\n",
    "- Reporte su notebook y anote sus soluciones a las preguntas planteadas. El reporte deberá contener un ensayo de [@KSKW2015] como introducción. Reporte los resultados de sus implementaciones, compare contra las alternativas presentadas en este reporte. Discuta sus resultados. Finalice el reporte  con reflexiones sobre el uso de nubes de puntos en lugar de bolsas de palabras tradicionales. Anoté sus conclusiones.\n",
    "\n",
    "# Bibliografía\n",
    "- [KSKW2015] Kusner, M., Sun, Y., Kolkin, N., & Weinberger, K. (2015, June). From word embeddings to document distances. In International conference on machine learning (pp. 957-966). PMLR.\n",
    "- [PW2009] Pele, O., & Werman, M. (2009, September). Fast and robust earth mover's distances. In 2009 IEEE 12th international conference on computer vision (pp. 460-467). IEEE.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
