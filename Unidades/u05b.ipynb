{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc189d2-656e-41d4-be84-dcf6ccf9537b",
   "metadata": {},
   "source": [
    "# Representación semántica y búsqueda\n",
    "Autor: Eric S. Tellez <eric.tellez@infotec.mx>\n",
    "\n",
    "Cuando la información esta poco específicada, las palabras adecuadas podrían ser dificiles de tener o limitar. En estos casos, la representaciones semánticas que permiten buscar _lo que se desea_ por medio de conceptos nos acerca más a la posibilidad de obtener información útil.\n",
    "\n",
    "Como anteriormente se presento, una de las representaciones semánticas más aceptadas son aquellas basadas en word embeddings. Recordando, estas son representaciones vectoriales de cada palabra, donde la semántica se asocia con la estructura en un espacio métrico, i.e., cercano en la métrica significa similar semánticamente.\n",
    "\n",
    "# Representación semántica de documentos basada en _word embeddings_\n",
    "Un documento puede verse como una bolsa de palabras, como en el modelado tradicional, pero en lugar de usar los términos como símbolos se pueden usar los vectores semánticos. El como usar esta nube de puntos multidimensional para obtener una representación computacionalmente manejable y a la vez eficaz, es un tema que ha llevado al desarrollo de modelos cada vez más complejos, como pueden ser los grandes modelos de lenguaje. \n",
    "\n",
    "La manera forma más directa de definir una representación semántica de documentos es el uso de centroides, esto es, un vector de la misma dimensión que sumariza a un conjunto de vectores usando su media geométrica. De manera más precisa, sea $E$ una matrix $m \\times n$ de embeddings del vocabulario, i.e., considerando un vocabulario de tamaño $n$ y dimensión $m$.\n",
    "Sea $D_{m,\\ell}$ la submatriz de $E$ que contiene los $\\ell$ vectores del documento $\\textsf{doc}$. El prototipo $\\vec{d}$ de $D$ esta definido de la siguiente forma:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{d} = \\frac{1}{\\ell} \\sum_{i=1}^\\ell D_i\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Donde $D_i$ es el i-ésimo vector columna de $D$. Dado que se usa el coseno como similitud, también es factible el cálculo como la suma vectorial normalizada, esto es,\n",
    "\n",
    "$$ \\vec{d} = \\frac{\\sum_{i=1}^\\ell D_i}{\\lVert \\sum_{i=1}^\\ell D_i \\rVert}$$\n",
    "\n",
    "También es posible añadir información local basada en la representación de bolsa de plabras. Por ejemplo, realizar una suma pesada usando el peso de las palabras mediante la frecuencia normalizada de término (TF) o la probabilidad de término $pt$, ver unidad 3 para más información.\n",
    "\n",
    "$$\\vec{d} = \\sum_{i=1}^{\\ell} \\textsf{TF}(t_i, \\textsf{doc}) \\cdot D_i$$\n",
    "\n",
    "Donde $t_i$ es el $i$-ésimo término de $\\textsf{doc}$. Note que el orden no se captura y solo es necesario para la notación.\n",
    "\n",
    "## Usando la nube de puntos\n",
    "\n",
    "La representación de nube de puntos puede usarse directamente sin realizar prototipos, sin embargo, es posible que dada su complejidad, dimensión intrinseca, tenga poca utilidad en grandes volumenes de información. Por ejemplo, es posible utilizar la distancia de [Hausdorff](https://en.wikipedia.org/wiki/Hausdorff_distance); que entre dos nubes de puntos (documentos para este caso) esta definida como sigue:\n",
    "\n",
    "$$ H_d(U, V) = \\max \\left\\{ \\max_{u \\in U} \\min_{x \\in V} d(u, x), \\max_{v \\in V} \\min_{x \\in U} d(v, x) \\right\\} $$\n",
    "\n",
    "Donde $\\textsf{nn}$ encuentra el vecino cercano del primer argumento en el segundo. Podemos ver la intuición de esta distancia, analizando las partes de la expresión. \n",
    "La diferencia entre ambos conjuntos se representa con el máximo de las distancias mínimas. Note que esta parte es idéntica a resolver una búsqueda de vecinos cercanos. La parte más externa de la expresión se repite para los dos conjuntos para preservar la simétria de la función.\n",
    "\n",
    "De la misma forma, es posible obtener variaciones de interés como considerar todas las distancias cercanas en lugar de solo las máximas, lo cual puede reducir formas caprichosas de _outlayers_.\n",
    "\n",
    "$$ H^+_d(U, V) = \\max \\left\\{ \\sum_{u \\in U} \\min_{x \\in V} d(u, x), \\sum_{v \\in V} \\min_{x \\in U} d(v, x) \\right\\}$$\n",
    "\n",
    "La información local de una representación de bolsa de palabras también puede ser aprovechada, por ejemplo, añadiendo información de $\\textsf{TF}$\n",
    "\n",
    "$$ H^\\textsf{TF}_d(U, V) = \\max \\left\\{ \\sum_{u \\in U} {\\textsf{TF}(u, U) \\min_{x \\in V} d(u, x)}, \\sum_{v \\in V} {\\textsf{TF}(v, V) \\min_{x \\in U} d(v, x)} \\right\\} $$\n",
    "\n",
    "# Notas adicionales\n",
    "La versión de $H_d$ que usa la probabilidad de término $\\textsf{pt}$ es similar a la aproximación 2 listada en [@KSKW2015]. Los prototipos son similares a la aproximación 1 del mismo artículo. En el artículo se presenta la función de distancia _word mover's distance_ (WMD) que es una adecuación de la Earths mover's distance (EMD) [PW2009]. EMD es la solución óptima a un problema de transportación, la WMD es la adaptación teniendo en cuenta embeddings de palabras.\n",
    "\n",
    "# Ejemplo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd04785e-aa4e-493c-9aa8-5c54c8e1f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "!isfile(\"Manifest.toml\") && Pkg.add([\n",
    "    PackageSpec(name=\"SimilaritySearch\", version=\"0.9\"),\n",
    "    PackageSpec(name=\"MLDatasets\", version=\"0.7\"),\n",
    "    PackageSpec(name=\"Plots\"),\n",
    "    PackageSpec(name=\"HypertextLiteral\"),\n",
    "    PackageSpec(name=\"Embeddings\", version=\"0.4\")\n",
    "])\n",
    "\n",
    "using SimilaritySearch, Plots, LinearAlgebra, Embeddings, HypertextLiteral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8bad4e-c31b-4506-8341-47c4130d664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_dataset()\n",
    "    emb = load_embeddings(GloVe{:en}, 2)  # you can change with any of the available embeddings in `Embeddings`\n",
    "    for c in eachcol(emb.embeddings)\n",
    "        normalize!(c)\n",
    "    end\n",
    "\n",
    "    db = MatrixDatabase(emb.embeddings)\n",
    "    db, emb.vocab\n",
    "end\n",
    "\n",
    "function create_index()\n",
    "    db, vocab = load_dataset()\n",
    "    dist = NormalizedCosineDistance()\n",
    "    index = SearchGraph(; dist, db, verbose=false)\n",
    "    index!(index)\n",
    "    optimize!(index, MinRecall(0.9))\n",
    "    index, vocab\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb60ff-68c0-4c09-a8dd-70079ba59994",
   "metadata": {},
   "source": [
    "### El índice métrico se crea, además se toma el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9108c0-02a7-4afe-bfa2-7b436f7bd0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time index, vocab = create_index()\n",
    "vocab2id = Dict(w => i for (i, w) in enumerate(vocab));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e629f24-ecf3-4908-a100-b70559dca89a",
   "metadata": {},
   "source": [
    "### Búsqueda de todos los vecinos cercanos en el vocabulario, observe la conveniencia del uso de un índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43463a8-0c73-4903-a78c-6b2405ddf812",
   "metadata": {},
   "outputs": [],
   "source": [
    "let k = 32\n",
    "    @time I, _ = allknn(index, k)\n",
    "    # WARNING: Don't run the following line, it takes too much time\n",
    "    @time gI, _ = allknn(ExhaustiveSearch(; db=index.db, dist=index.dist), k)\n",
    "    macrorecall(gI, I)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b68d72-5256-49ad-b22b-e896f58bc29f",
   "metadata": {},
   "source": [
    "### Búsqueda y presentación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045135e-4189-47cc-9513-72a5000527f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "function search_and_display(index, vocab, q, res, k, qword)\n",
    "    res = reuse!(res, k)\n",
    "    @time search(index, q, res)\n",
    "\n",
    "    L = []\n",
    "    for (j, (id, d)) in enumerate(res)\n",
    "        push!(L, @htl \"<tr><td>$j</td><td>$(vocab[id])</td><td>$id</td><td>$(round(d, digits=3))</td></tr>\")\n",
    "    end\n",
    "\n",
    "    display(@htl \"\"\"<h2>resultados for \"$qword\"</h2>\n",
    "    <table>\n",
    "    <th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
    "        $L\n",
    "    </table>\n",
    "    \"\"\")\n",
    "end\n",
    "\n",
    "function search_and_display(index, vocab, qid::Integer, res, k=maxlength(res))\n",
    "    search_and_display(index, vocab, index[qid], res, k, vocab[qid])\n",
    "end\n",
    "\n",
    "display(@htl \"<h1>Ejemplos de búsqueda (aleatorios)</h1>\")\n",
    "res = KnnResult(7)\n",
    "for i in 1:3\n",
    "    for qid in rand(1:length(vocab))\n",
    "        search_and_display(index, vocab, qid, res)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b320eacd-d47e-4dbe-a7e9-a240c4535b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_and_display(index, vocab, vocab2id[\"glove\"], res, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e60967-b159-4e46-a0a1-369efbe70477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0d1ea-bdf8-468a-a20d-1bdf0c7a6503",
   "metadata": {},
   "outputs": [],
   "source": [
    "function analogy(a, b, c, k)\n",
    "\tv = index[vocab2id[a]] - index[vocab2id[b]] + index[vocab2id[c]]\n",
    "\tnormalize!(v)\n",
    "\tsearch_and_display(index, vocab, v, res, k, \"<$a> - <$b> + <$c>\")\n",
    "end\n",
    "\n",
    "analogy(\"father\", \"man\", \"woman\", 5)\n",
    "analogy(\"fireman\", \"man\", \"woman\", 5)\n",
    "analogy(\"policeman\", \"man\", \"woman\", 5)\n",
    "analogy(\"mississippi\", \"usa\", \"france\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f09a9-75aa-49c5-9e35-4c486defa289",
   "metadata": {},
   "source": [
    "# Actividades\n",
    "- Defina $H^{pt}_d$, donde $pt$ es la probabilidad de ocurrencia de término.\n",
    "- Reproduzca el ejercicio de este notebook, use embeddings para español, cambié los ejemplos. Se sugiere el uso de <https://ingeotec.github.io/regional-spanish-models/> donde encontrará modelos fastText regionalizados del español, pero puede usar otros embeddings.\n",
    "- Implemente $H^\\textsf{TF}_d$ y $H^\\textsf{pt}_d$ de manera secuencial y con un índice métrico. Si usa Julia considere `SimilaritySearch.jl` y si usa Python considere `faiss`.\n",
    "- Reporte su notebook y anote sus soluciones a las preguntas planteadas. El reporte deberá contener un ensayo de [@KSKW2015] como introducción. Reporte los resultados de sus implmentaciones, compare contra las alternativas presentadas en este reporte. Discuta sus resultados. Finalice el reporte  con reflexiones sobre el uso de nubes de puntos en lugar de bolsas de palabras tradicionales. Anoté sus conclusiones.\n",
    "\n",
    "# Bibliografía\n",
    "- [KSKW2015] Kusner, M., Sun, Y., Kolkin, N., & Weinberger, K. (2015, June). From word embeddings to document distances. In International conference on machine learning (pp. 957-966). PMLR.\n",
    "- [PW2009] Pele, O., & Werman, M. (2009, September). Fast and robust earth mover's distances. In 2009 IEEE 12th international conference on computer vision (pp. 460-467). IEEE.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
