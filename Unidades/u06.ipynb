{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ac4373-ac6b-41a1-b90c-0f7e8dbcf2bc",
   "metadata": {},
   "source": [
    "# Agrupamiento y visualización de información textual\n",
    "\n",
    "Autor: Eric S. Tellez <eric.tellez@infotec.mx> <br/>\n",
    "\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Hasta el momento hemos visto operaciones de búsqueda de texto completo y de la búsqueda en espacios métricos.\n",
    "Ambas operaciones son fundamentales en un sistema de recuperación de información, y de hecho, en otras áreas como son la inteligencia computacional.\n",
    "También se vió la clasificación basada en vecinos cercanos como una herramienta para comparar modelos de texto.\n",
    "\n",
    "Recordemos que el objetivo de la disciplina de Recuperación de Información es facilitar el acceso a las personas a grandes cúmulos de información,\n",
    "de tal forma que seamos capaces de encontrar la información sobre la que se realizan consultas. En ese espíritu, el agrupamiento (clustering) y la visualización son de gran ayuda para obtener información de valor.\n",
    "\n",
    "En el agrupamiento se parte una base de datos métrica (documentos o cualquier otro tipo de dato) de tal forma que los elementos en cada una de sus partes tengan cierta afinidad, por ejemplo que sean cercanos y compactos, o que pertenezcan a un grupo donde la densidad sea alta. Normalmente se intenta reducir una función de costo para cada grupo y de manera global.\n",
    "\n",
    "Dado que los documentos suelen estar representados en muy alta dimensión, es prácticamente imposible visualizar y aprender sobre la estructura de una base de datos. Esto último lo podemos realizar mediante transformaciones que nos permitan obtener dimensiones que permitan la visualización de las estructuras subyacientes, i.e., dimensiones 2 y 3.\n",
    "\n",
    "Recuerde que en la representación basada en bolsas de palabras es común encontrar vectores de decenas o cientos de miles de dimensiones, aunque sumamente dispersos. En las representaciones semánticas se encuentran en espacios densos de cientos de dimensiones. La distancia en ambos casos es comúnmente medida en términos de los ángulos de los vectores, ya que esto nos permité disociar las magnitudes que cambiarán con la cantidad de componentes y con el tamaño de los documentos. Lo último nos da ventajas ya que normalmente se desea recuperar documentos largos a partir de consultas pequeñas.\n",
    "\n",
    "## Agrupamiento (clustering)\n",
    "\n",
    "Existen diversos algoritmos de clustering que pueden apoyar en la tarea, todos ellos con diferentes características que lo hacen adecuado para ciertas tareas.\n",
    "En particular, podemos mencionar algunos ejemplos paradigmáticos, como sigue:\n",
    "\n",
    "- [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering): Intenta encontrar los $k$ centros $C$ que minimicen la suma de las distancias al cuadrado de todos los elementos de la base de datos a algún elemento en $C$. En su forma más simple, el algoritmo comienza con una muestra aleatoria de $k$ puntos y usa el [algoritmo de Lloyd](https://en.wikipedia.org/wiki/Lloyd%27s_algorithm) para refinar los $k$ puntos: en cada iteración, se genera una partición con los elementos más cercanos a cada uno de los centroides, y se usan para generar nuevos centroides. El algoritmo convergerá en algunas iteraciones ya que es bastante rápido. Tiene como desventaja que el usuario debe saber el número de grupos $k$ óptimo y que su calidad puede variar de manera dramática con diferentes formas de inicializar el algoritmo.\n",
    "\n",
    "- [affinity propagation](https://en.wikipedia.org/wiki/Affinity_propagation): Usa tres matrices y un esquema basado en mensajes para obtener los elementos más adecuados para representar una base de datos. En este algoritmo no es necesario especificar $k$ ya que se calcula como parte del proceso. Las tres matrices son, la matriz de similitud, la matriz de reponsabilidad y de de disponibilidad. La idea es considerar a todos los elementos como posibles centros e intercambiar mensajes entre las matrices de responsabilidad y disponibilidad, usando la matriz de similitud como base de la información. El algoritmo es iterativo y se detendrá cuando no haya cambios en las matrices o se llegue a un límite en el número de las iteraciones. La estructura del cluster emergerá como parte este proceso.\n",
    "\n",
    "- [DBSCAN](https://es.wikipedia.org/wiki/DBSCAN): Es un algoritmo que es especial para descubrir grupos en base a la densidad, de tal forma que para que dos puntos $u$ y $v$ pertenezcan al mismo grupo, no necesariamente son cercanos entre ellos, pero si que hay una serie de puntos $\\{w_i\\}$ en el mismo grupo con los que se puede trazar una ruta entre $u$ y $v$. Ahora, para que un elemento forme parte de un grupo debe cumplir que comparte al menos $\\ell$ elementos en común con el grupo en un radio de búsqueda $\\epsilon$, los puntos dentro de una ruta entonces cumplen con estas propiedades. A diferencia de los algoritmos basados en particiones compactas, las formas de los grupos en DBSCAN pueden ser caprichosas. No es necesario conocer el número de centros pero si $\\ell$ y $\\epsilon$.\n",
    "\n",
    "## Proyecciones a baja dimensión\n",
    "Una de las herramientas más importantes para el entendimiento de una colección de objetos, llamese documento o cualquier otro tipo de dato, es la visualización. La visualización consiste en obtener proyecciones en 2D o 3D y utilizar diferentes estrategias para que la información de valor sea _evidente_ a nuestros ojos.\n",
    "\n",
    "En particular, podemos considerar dos formas primordiales de proyecciones: lineales y no-lineales. Las proyecciones lineales son eficientes de calcular pero solo podrán capturar información lineal en la proyección. Un ejemplo paradigmático es Principal Compontent Analysis ([PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)), que es ampliamente usado en ciencia de datos y que la gran mayoría de la paquetería para análisis de datos lo implementará ([PCA en sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), [PCA en Julia](https://github.com/JuliaStats/MultivariateStats.jl)). En particular, cuando los datos podrían no ser proyectados de manera adecuada con transformaciones lineales, entonces se hace necesario el uso de transformaciones no lineales. En este caso podemos tener en cuenta Kernel PCA ([KPCA](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis)) cuya no linearidad es manejada por funciones kernel. En este caso, la familia de métodos _manifold learning_ también pueden ser una opción. En estos casos, en lugar de analizar las componentes de los vectores, se hace uso de la matriz de afinidad $A$, i.e., matriz de distancias $d(u, v) \\forall u, v \\in X$, o el grafo de todos los $k$ vecinos cercanos $G$. En particular, estos métodos intentarán crear representaciones vectoriales de baja dimensión\n",
    "que repreduzcan hasta cierto nivel de precisión dicha matriz $A$ o $G$, esto es, que las relaciones topológicas se preserven. Entre estos métodos podemos encontrar [Isomap](https://en.wikipedia.org/wiki/Isomap), [tSNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) o [UMAP](https://umap-learn.readthedocs.io/en/latest/).\n",
    "\n",
    "En particular, es posible encontrar implementaciones de ellos en [sklearn](https://scikit-learn.org/stable/modules/manifold.html) y [UMAP](https://umap-learn.readthedocs.io/en/latest/) para Python; en Julia [ManifoldLearning.jl](https://github.com/wildart/ManifoldLearning.jl) y [SimSearchManifoldLearning.jl](https://github.com/sadit/SimSearchManifoldLearning.jl), donde también se puede acceder a una implementación eficiente de UMAP.\n",
    "\n",
    "# Bibliografia\n",
    "- [] McInnes, L., Healy, J., & Melville, J. (2018). Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426.\n",
    "# Ejemplo\n",
    "\n",
    "Uso de búsqueda por similitud para recuperar imagenes similares, de manera más precisa usando [FashionMNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a128ea0-9690-44d6-85cd-67b344479cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/IR-2022/Unidades`\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Package SimSearchManifoldLearning not found in current path:\n- Run `import Pkg; Pkg.add(\"SimSearchManifoldLearning\")` to install the SimSearchManifoldLearning package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package SimSearchManifoldLearning not found in current path:\n- Run `import Pkg; Pkg.add(\"SimSearchManifoldLearning\")` to install the SimSearchManifoldLearning package.\n",
      "",
      "Stacktrace:",
      " [1] require(into::Module, mod::Symbol)",
      "   @ Base ./loading.jl:967",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "!isfile(\"Manifest.toml\") && Pkg.add([\n",
    "    PackageSpec(name=\"SimilaritySearch\", version=\"0.9\"),\n",
    "    PackageSpec(name=\"SimSearchManifoldLearning\", version=\"0.2\"),\n",
    "    PackageSpec(name=\"Embeddings\"),\n",
    "    PackageSpec(name=\"Plots\"),\n",
    "    PackageSpec(name=\"HypertextLiteral\"),\n",
    "])\n",
    "\n",
    "using SimilaritySearch, SimSearchManifoldLearning, Embeddings, Plots, LinearAlgebra, MLDatasets, HypertextLiteral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48a8f4-b09a-4413-b347-37e2ac461413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ec627-3d8e-46a6-8e85-025a02a2ed07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a893ee-cc8f-46b0-9710-9f199ef9eb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36614caa-da49-4f8c-924d-fef0b73f3012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701dfb4-180a-45da-b445-63bbe7c195f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e895de-a0b2-48be-9a5b-64444bc6797c",
   "metadata": {},
   "source": [
    "# Desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89a19b-bf82-4dbe-a305-a35fdbd0b2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf9c5a-aacb-432d-b2d3-6a37673b3934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d31086d-48ab-4129-b88d-27f35dad66e4",
   "metadata": {},
   "source": [
    "## Ahora haciendo una consulta más amplia, los 32 vecinos cercanos de todos los elementos de la db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c7bd5-3568-4625-a2a2-eb08b4433faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71b8ce28-af7b-4764-9c36-8f01a08afba6",
   "metadata": {},
   "source": [
    "Mientras más consultas se hagan, los costos iniciales del preprocesamiento se amortizarán y la ganancia se verá más evidente, aun con la pequeña perdida de calidad. Así mismo, mientras más consultas se hacen, el costo de una búsqueda exhaustiva se incrementa de manera significativa.\n",
    "\n",
    "# Actividades\n",
    "- ¿Cuáles son las propiedades de una función métrica? discuta como cada una de ellas se pueden usar en la búsqueda.\n",
    "- Con respecto a la desigualdad del triángulo, describa como se utiliza para acelerar una búsqueda.\n",
    "- Si usa Julia, revise el paquete `SimilaritySearch.jl`: <https://github.com/sadit/SimilaritySearch.jl> y <https://github.com/sadit/SimilaritySearchDemos>\n",
    "- Si usa Python, revise el paquete FAISS: <https://github.com/facebookresearch/faiss> y <https://faiss.ai/>\n",
    "- Implemente una búsqueda métrica exhaustiva que acepté como argumento la función de distancia\n",
    "- Reproduzca el ejercicio de este notebook, si usa Julia use, adicionalmente al ejemplo, los conjunto de datos generados en la U2 (modelo completo y modelo filtrado).\n",
    "- Reporte su notebook con una comparación de tiempos y calidades, anote sus reflexiones sobre los modelos de datos, dominios y los alcances de los algoritmos de búsqueda métrica.\n",
    "\n",
    "# Bibliografía\n",
    "\n",
    "- [@AFN2004] Alber, J., Fellows, M. R., & Niedermeier, R. (2004). Polynomial-time data reduction for dominating set. Journal of the ACM (JACM), 51(3), 363-384.\n",
    "- [@AW2019] Amid, E., & Warmuth, M. K. (2019). TriMap: Large-scale dimensionality reduction using triplets. arXiv preprint arXiv:1910.00204.\n",
    "- [@CNBYM2001] Chávez, E., Navarro, G., Baeza-Yates, R., & Marroquín, J. L. (2001). Searching in metric spaces. ACM computing surveys (CSUR), 33(3), 273-321.\n",
    "- [@CT2010] Chávez, E., & Tellez, E. S. (2010, September). Navigating k-nearest neighbor graphs to solve nearest neighbor searches. In Mexican Conference on Pattern Recognition (pp. 270-280). Springer, Berlin, Heidelberg.\n",
    "- [@DS2020] DS, D. (2020). A simple solution for the taxonomy enrichment task: Discovering hypernyms using nearest neighbor search.\n",
    "- [@FH2017] Faessler, E., & Hahn, U. (2017, July). Semedico: a comprehensive semantic search engine for the life sciences. In Proceedings of ACL 2017, System Demonstrations (pp. 91-96).\n",
    "- [@GCVR2018] Gallego, A. J., Calvo-Zaragoza, J., Valero-Mas, J. J., & Rico-Juan, J. R. (2018). Clustering-based k-nearest neighbor classification for large-scale data with neural codes representation. Pattern Recognition, 74, 531-543.\n",
    "- [@GLS2008] Goyal, N., Lifshits, Y., & Schütze, H. (2008, February). Disorder inequality: a combinatorial approach to nearest neighbor search. In Proceedings of the 2008 international conference on web search and data mining (pp. 25-32).\n",
    "- [@HN2014] Houle, M. E., & Nett, M. (2014). Rank-based similarity search: Reducing the dimensional dependence. IEEE transactions on pattern analysis and machine intelligence, 37(1), 136-150.\n",
    "- [@Hetland2020] Hetland, M. L. (2020, September). Optimal Metric Search Is Equivalent to the Minimum Dominating Set Problem. In International Conference on Similarity Search and Applications (pp. 111-125). Springer, Cham.\n",
    "- [@LV2017] Lee, J. A., & Verleysen, M. (2007). Nonlinear dimensionality reduction (Vol. 1). New York: Springer.\n",
    "- [@MHM2018] McInnes, L., Healy, J., & Melville, J. (2018). Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426.\n",
    "- [@MPLK2014] Malkov, Y., Ponomarenko, A., Logvinov, A., & Krylov, V. (2014). Approximate nearest neighbor algorithm based on navigable small world graphs. Information Systems, 45, 61-68.\n",
    "- [@MYD2018] Malkov, Y. A., & Yashunin, D. A. (2018). Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4), 824-836.\n",
    "- [@OTGMM2020] Ortiz-Bejar, J., Téllez, E. S., Graff, M., Moctezuma, D., & Miranda-Jiménez, S. (2020). Improving k Nearest Neighbors and Naïve Bayes Classifiers Through Space Transformations and Model Selection. IEEE Access, 8, 221669-221688.\n",
    "- [@RCGT2015] Ruiz, G., Chávez, E., Graff, M., & Téllez, E. S. (2015, October). Finding near neighbors through local search. In International Conference on Similarity Search and Applications (pp. 103-109). Springer, Cham.\n",
    "- [@RUB2018] Rubinstein, A. (2018, June). Hardness of approximate nearest neighbor search. In Proceedings of the 50th annual ACM SIGACT symposium on theory of computing (pp. 1260-1268).\n",
    "- [@SKL2011] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. the Journal of machine Learning research, 12, 2825-2830.\n",
    "- [@SPA2019] Soto, A. J., Przybyła, P., & Ananiadou, S. (2019). Thalia: semantic search engine for biomedical abstracts. Bioinformatics, 35(10), 1799-1801.\n",
    "- [@SS2021] Sharma, K. K., & Seal, A. (2021). Spectral embedded generalized mean based k-nearest neighbors clustering with S-distance. Expert Systems with Applications, 169, 114326.\n",
    "- [@TR2022] Tellez, E. S., & Ruiz, G. (2022). Similarity search on neighbor's graphs with automatic Pareto optimal performance and minimum expected quality setups based on hyperparameter optimization. arXiv preprint arXiv:2201.07917.\n",
    "- [@TRCG2021] Tellez, E. S., Ruiz, G., Chavez, E., & Graff, M. (2021). A scalable solution to the nearest neighbor search problem through local-search methods on neighbor graphs. Pattern Analysis and Applications, 24(2), 763-777.\n",
    "- [@VMH2018] Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning research, 9(11).\n",
    "- [@WPAA2017] Wachsmuth, H., Potthast, M., Al Khatib, K., Ajjour, Y., Puschmann, J., Qu, J., ... & Stein, B. (2017, September). Building an argument search engine for the web. In Proceedings of the 4th Workshop on Argument Mining (pp. 49-59).\n",
    "- [@YCC2020] Yu, Q., Chen, K. H., & Chen, J. J. (2020, September). Using a set of triangle inequalities to accelerate k-means clustering. In International Conference on Similarity Search and Applications (pp. 297-311). Springer, Cham."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
