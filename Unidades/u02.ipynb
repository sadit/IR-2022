{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b606b63e-967d-4d4d-85e4-e02a32020425",
   "metadata": {},
   "source": [
    "# Búsqueda de texto completo\n",
    "Autor: Eric S. Tellez <eric.tellez@infotec.mx> <br/>\n",
    "\n",
    "Tal vez la tarea más emblemática de la Recuperación de Información es la búsqueda de _texto completo_.\n",
    "El problema consiste en dado un corpus grande de documentos, preprocesarlo para crear una estructura de búsqueda que permita resolver consultas de manera eficiente. Una consulta es un texto corto que específica lo que se desea encontrar en la colección. En particular, es un ejemplo de lo que se desea. Esto lleva a que la estructura de búsqueda resuelve búsquedas por similitud.\n",
    "\n",
    "La similitud, es entonces un tema central, pero para medirla lo primero es tener una representación de los datos que capture las propiedades deseadas (que serán después evaluadas para medir la similitud). La manera más tradicional de hacerlo, es el uso de un modelo basado en bolsa de palabras (BOW). En dicho modelo, el texto es preprocesado, toquenizado y vectorizado.\n",
    "\n",
    "- El preprocesamiento incluye tratamientos tan simples como eliminar símbolos no deseados, eliminiación de variantes léxicas, reducción a raíces o lemas, corrección de ortografía, eliminiación de palabras comunes (stop words). \n",
    "- El toquenizado es el proceso donde el texto es partido, en frases u oraciones, y finalmente en palabras y símbolos que son unidades completas. En este punto también es posible realizar normalizaciones, así como también realizar limpieza basada en estadísticas de los términos.\n",
    "- El vectorizado utiliza el vocabulario de una colección $\\{t_i\\}$ para generar una matriz de la colección, i.e., un vector por documento.\n",
    "\n",
    "Al proceso de modelar una colección mediante un vocabulario y luego ser capaces de generar una representación manejable por una computadora se le llama _modelo de lenguaje_.\n",
    "\n",
    "## Problema de búsqueda\n",
    "Una vez que se genero el modelo de lenguaje y que fue usado para vectorizar una colección $X$, la idea es ser capaces de resolver consultas $Q$, i.e., encontrar un subconjunto de $X$ que mejor se apegue a una especificación $q \\in Q$. Las consultas deben ser codificadas de la misma forma, para generar un vector con ellas. Entonces el problema se transforma en encontrar los elementos más parecidos, que dada la representación, es conveniente usar el coseno entre vectores:\n",
    "\n",
    "$$ \\cos(u, q) = \\frac{ \\sum_i {u_i \\cdot q_i}}{\\sqrt{\\sum_i u_i^2} \\cdot \\sqrt{\\sum_i q_i^2}} $$\n",
    "\n",
    "Así mismo, $d(u, q) = \\arccos(\\cos(u, q))$ sería el ángulo entre ambos vectores, que además es una métrica. El problema entonces se transforma en encontrar los vecinos más cercanos en la colección, esto es, si deseamos $k$ resultados de una consulta, estaríamos deseando encontrar aquel subconjunto $knn$ de la colección tal que $\\sum_{v \\in knn} d(v, q)$ sea mínimo comparado con todo subconjunto de tamaño $k$ de la colección de documentos.\n",
    "\n",
    "## Velocidad de consultas\n",
    "Para mejorar la solución de consultas, es posible crear una estructura de datos que simplifique el proceso de encontrar el subconjunto $knn$. En este problema, con una representación basada en bolsa de palabras, la estructura más adecuada es el _índice invertido.\n",
    "\n",
    "\n",
    "# Índice invertido\n",
    "\n",
    "Un índice invertido es una representación dispersa de la matriz $W_{m,n}$ formada por $m$ componentes y $n$ documentos, i.e., cada celda $w_{t,i}$ es el peso asignado para el término $t$ que ocurre en el documento $i$. Por construcción, esta matriz tiene una gran cantidad de ceros, por lo que $W$ es altamente dispersa (pocos términos ocurren en un documento).\n",
    "\n",
    "$$ W \\left \\{\n",
    "\\begin{array}{rrrr rrrr rr}\n",
    "                & \\vec x_1& \\vec x_2&       & \\vec x_n \\\\\n",
    "t_1 \\rightarrow & w_{1,1} & w_{1,2} & \\dots & w_{1,n} \\\\\n",
    "t_2 \\rightarrow & w_{2,1} & w_{2,2} &       & w_{2,n} \\\\\n",
    "                & \\vdots  &         & \\ddots&         \\\\\n",
    "t_m \\rightarrow & w_{m,1} & w_{m,2} &       & w_{m,n} \\\\\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$\n",
    "\n",
    "La representación es entonces por fila, a manera de lista de adjacencia; esto es, cada fila $t$ es representada por las tuplas $(i, w_{t,i})$, esto es, un índice invertido es la siguiente estructura $W^*$\n",
    "\n",
    "$$ W^* \\left \\{\n",
    "\\begin{array}{rrr}\n",
    "t_1 & \\rightarrow & \\{(i, w_{1, i})\\} \\\\\n",
    "t_2 & \\rightarrow & \\{(i, w_{2, i})\\} \\\\\n",
    "\\vdots & \\vdots   &  \\hfill \\vdots \\hfill \\\\\n",
    "t_m & \\rightarrow & \\{(i, w_{m, i})\\} \\\\\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$\n",
    "\n",
    "la tupla es usada siempre y cuando $w > 0$. Las tuplas suelen ordenarse por su identificador de columna, pero también puede usarse el peso según convenga. A las filas suele llamarseles listas de posteo (_posting lists_). Los requerimientos de una matriz densa son altísimos para representaciones de texto de alta dimensión, representar las matrices de manera dispersa simplifica el manejo de la memoría, y como se verá a continuación, también influye enormemente en los tiempos de procesamiento.\n",
    "\n",
    "### Búsqueda mediante un índice invertido\n",
    "\n",
    "La solución na\\\"ive de una obtener los $k$ documentos más similares es evaluar todos los vectores $\\vec{x}_i$, i.e., columnas de $W$, y determinar aquellos más similares, i.e., minimizar $d(\\vec{x}_i, q)$.\n",
    "\n",
    "El índice invertido $W^*$ contiene la información necesaria para realizar esta operación de manera eficiente. Primeramente, es necesario analizar la expresión de $\\cos$. El denominador $\\sqrt{\\sum_i u_i^2} \\cdot \\sqrt{\\sum_i q_i^2}$, en sus partes es estático para cada vector, por lo que se puede preprocesar y no calcular de manera explícita para cada evaluación de $\\cos$. Con respecto al numerador corresponde al producto punto entre $\\vec{u}$ y $\\vec{q}$, $\\sum_i u_i \\cdot q_i$. Dicho esto, solo es necesario calcular los productos diferentes de cero; así pues, la evaluación eficiente de $\\cos$ corresponde con una evaluación eficiente de la intersección de las componentes diferentes de cero. Los algoritmos como SvS, BY o BK, pueden ser de gran ayuda para este cálculo. Note que aunque que los pesos con valor cero no se representan en $W^*$, dicho índice representa información por fila, lo cual no permite hacer operaciones eficientes entre $q$ y los vectores columna $\\vec x$ individuales.\n",
    "\n",
    "\n",
    "Afortunadamente, la evaluación se puede hacer eficiente para todo el conjunto de posibles candidatos (aquellos donde el producto punto contra $q$ sea diferente de cero). Para esto, se toman las componentes diferentes de cero en $q$, se toman las listas de adyacencia de $W^*$ y se procede a unirlas de manera eficiente. El conjunto de identificadores de documento resultado de esta unión será aquel que debe ser evaluado para obtener el conjunto de documentos similares. \n",
    "Si uno toma la intersección, que puede ser más veloz de calcular, entonces podrían perderse documentos relavantes; es posible también mandar el problema a un punto intermedio, es decir al problema de $t$-thresholds, donde se recupera un conjunto donde cada uno de los miembros aparece en al menos $t$ listas.\n",
    "La manera más eficiente, sin embargo, es realizar optimizaciones por filtrado de pesos o mejorando los esquemas de pesado; la idea general entonces es desaparecer entradas de $W*$ de tal forma que la unión sea siempre pequeña. La adecuada optimización de un índice invertido puede hacerlo escalable a niveles realmente impresionantes.\n",
    "\n",
    "Los algoritmos de BK pueden ser utilizados para calcular la unión y t-threasholds, así como los algoritmos de mezcla clásicos (_merge_). Es posible unir la operación de unión con la operación de producto punto por vector usando los algoritmos adecuados.\n",
    "\n",
    "- <https://github.com/sadit/InvertedFiles.jl/blob/main/src/invfilesearch.jl>\n",
    "- <https://github.com/sadit/InvertedFiles.jl/blob/main/src/winvfilesearch.jl>\n",
    "- <https://github.com/sadit/Intersections.jl/blob/main/src/merge.jl>\n",
    "\n",
    "# Ejemplo\n",
    "\n",
    "El siguiente es un ejemplo de como cambia el desempeño usando una evaluación exhaustiva, un índice invertido y una pequeña optimización basada en modificación de pesos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb11ca12-b635-45bb-ac02-14523674ded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/IR-2022/Unidades`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "\n",
    "!isfile(\"Manifest.toml\") && Pkg.add([\n",
    "    PackageSpec(name=\"SimilaritySearch\", version=\"0.9\"),\n",
    "    PackageSpec(name=\"TextSearch\", version=\"0.12\"),\n",
    "    PackageSpec(name=\"InvertedFiles\", version=\"0.4\"),\n",
    "    PackageSpec(name=\"CodecZlib\", version=\"0.7\"),\n",
    "    PackageSpec(name=\"JSON\", version=\"0.21\"),\n",
    "    PackageSpec(name=\"HypertextLiteral\", version=\"0.9\")\n",
    "])\n",
    "\n",
    "using TextSearch, InvertedFiles, SimilaritySearch, TextSearch, CodecZlib, JSON, LinearAlgebra, HypertextLiteral\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd994e-6be8-45c8-a0fc-8f05dfcf2636",
   "metadata": {},
   "source": [
    "## Funciones para leer y modelar el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9482f05-8d88-4896-9b9e-646bc1da2abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_dataset (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function parse_corpus(corpusfile)\n",
    "    corpus, labels = String[], String[]\n",
    "    open(corpusfile) do f\n",
    "        for line in eachline(GzipDecompressorStream(f))\n",
    "            r = JSON.parse(line)\n",
    "            push!(labels, r[\"klass\"])\n",
    "            push!(corpus, r[\"text\"])    \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    corpus, labels\n",
    "end\n",
    "\n",
    "function text_model_and_vectors(\n",
    "        corpus;\n",
    "        textconfig=TextConfig(group_usr=true, group_url=true, del_diac=true, lc=true, group_num=true, nlist=[1], qlist=[]),\n",
    "        model=VectorModel(IdfWeighting(), TfWeighting(), textconfig, corpus)\n",
    "    )\n",
    "    vectors = vectorize_corpus(model, textconfig, corpus)\n",
    "    for v in vectors\n",
    "        normalize!(v)\n",
    "    end\n",
    "\n",
    "    (; textconfig, model, vectors)\n",
    "end\n",
    "\n",
    "function create_dataset(corpusfile)\n",
    "    corpus, labels = parse_corpus(corpusfile)\n",
    "    (; labels, corpus, text_model_and_vectors(corpus)...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1176ef00-1153-4cba-bb89-354d634c857e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Cargando el corpus (descargado en U1.ipynb)</h1>"
      ],
      "text/plain": [
       "<h1>Cargando el corpus (descargado en U1.ipynb)</h1>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(unique(D.labels), D.model) = ([\"😰\", \"😥\", \"😊\", \"😏\", \"♡\", \"💔\", \"🙂\", \"😋\", \"😌\", \"🌚\", \"👌\", \"😪\", \"😤\", \"🙃\", \"🤤\", \"😴\", \"😢\", \"😅\", \"😑\", \"😠\", \"😂\", \"😜\", \"🤓\", \"💙\", \"😀\", \"🤗\", \"🤣\", \"😒\", \"✨\", \"😐\", \"😞\", \"😁\", \"😱\", \"👏\", \"😫\", \"😍\", \"❤\", \"😣\", \"🙊\", \"🙏\", \"🙄\", \"🤭\", \"💜\", \"🤔\", \"😬\", \"👀\", \"😉\", \"😈\", \"😡\", \"😳\", \"🙈\", \"😻\", \"😔\", \"😓\", \"💕\", \"🎶\", \"😭\", \"😕\", \"♥\", \"💖\", \"😎\", \"😘\", \"😃\", \"😩\"], {VectorModel global_weighting=IdfWeighting(), local_weighting=TfWeighting(), train-voc=45374, train-n=50000, maxoccs=93991})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([\"😰\", \"😥\", \"😊\", \"😏\", \"♡\", \"💔\", \"🙂\", \"😋\", \"😌\", \"🌚\"  …  \"💕\", \"🎶\", \"😭\", \"😕\", \"♥\", \"💖\", \"😎\", \"😘\", \"😃\", \"😩\"], {VectorModel global_weighting=IdfWeighting(), local_weighting=TfWeighting(), train-voc=45374, train-n=50000, maxoccs=93991})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(@htl \"<h1>Cargando el corpus (descargado en U1.ipynb)</h1>\")\n",
    "dbfile = \"../data/emo50k.json.gz\"\n",
    "D = create_dataset(dbfile)\n",
    "@show unique(D.labels), D.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8671092-ee09-4370-b732-4d40d3d4b7b1",
   "metadata": {},
   "source": [
    "# Creando los diferentes métodos de búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd7ccd91-c1c2-4a04-b792-d38a8c65342a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{WeightedInvertedFile vocsize=3968, n=50000}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# índice invertido\n",
    "invfile = WeightedInvertedFile(length(D.model.voc))\n",
    "append!(invfile, VectorDatabase(D.vectors))\n",
    "\n",
    "## indice invertido con filtros al vocabulario\n",
    "fmodel = filter_tokens(D.model) do t\n",
    "    10 <= t.ndocs <= 1000  # filtrando los tokens que ocurren en entre 10 y 1000 documentos\n",
    "end\n",
    "\n",
    "## usando el modelo reducido de tokens\n",
    "fD = text_model_and_vectors(D.corpus, textconfig=D.textconfig, model=fmodel)\n",
    "\n",
    "finvfile = WeightedInvertedFile(length(fD.model.voc))\n",
    "append!(finvfile, VectorDatabase(fD.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d118cb7-f81d-4ad1-80fd-4a5245098ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The token filters can operate on any valid field of voc (passed as the named tuple <em>t</em>)"
      ],
      "text/plain": [
       "The token filters can operate on any valid field of voc (passed as the named tuple <em>t</em>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary <: Any\n",
      "  token::Vector{String}\n",
      "  occs::Vector{Int32}\n",
      "  ndocs::Vector{Int32}\n",
      "  weight::Vector{Float32}\n",
      "  token2id::Dict{String, UInt32}\n",
      "  corpuslen::Int64\n"
     ]
    }
   ],
   "source": [
    "display(@htl \"The token filters can operate on any valid field of voc (passed as the named tuple <em>t</em>)\")\n",
    "\n",
    "dump(typeof(D.model.voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "046760fa-03d2-4bc9-a6cd-418c4f655b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorDatabase{Vector{Dict{UInt32, Float32}}}(Dict{UInt32, Float32}[Dict(0x00000054 => 0.3347092, 0x0000068a => 0.6846173, 0x00000044 => 0.34018824, 0x00000419 => 0.5509455), Dict(0x00000186 => 0.7395449, 0x000000a2 => 0.6731072), Dict(0x000000b0 => 0.23381734, 0x00000053 => 0.3259642, 0x000000e7 => 0.3169817, 0x0000001b => 0.2925202, 0x00000d35 => 0.49764267, 0x000001f5 => 0.34775817, 0x00000cb9 => 0.42758507, 0x000000e8 => 0.3187763), Dict(0x000000b0 => 0.3558882, 0x00000eef => 0.74962527, 0x000002e8 => 0.5580372), Dict(0x0000016b => 0.2604656, 0x0000004f => 0.23028766, 0x00000448 => 0.4283243, 0x000000c9 => 0.26923087, 0x000003eb => 0.44006115, 0x00000365 => 0.3291022, 0x000007af => 0.42488283, 0x00000a58 => 0.37508774), Dict(0x00000259 => 0.39137, 0x00000dd7 => 0.7243029, 0x000000bb => 0.5676397), Dict(0x00000084 => 0.4996388, 0x00000d45 => 0.8662338), Dict(0x00000487 => 0.22856358, 0x00000bf3 => 0.2630118, 0x000006a7 => 0.23114121, 0x00000293 => 0.2616991, 0x00000916 => 0.25267273, 0x00000b2c => 0.27989882, 0x000000a6 => 0.17348991, 0x00000817 => 0.25578964, 0x000009e4 => 0.23695171, 0x00000054 => 0.13684246…), Dict(0x000000a9 => 0.36354518, 0x00000a07 => 0.5120805, 0x000002bd => 0.45747685, 0x0000017e => 0.5208684, 0x0000017d => 0.35358092), Dict(0x0000038c => 0.52716184, 0x00000157 => 0.32211804, 0x0000046a => 0.4700343, 0x00000076 => 0.32827374, 0x0000024d => 0.3729686, 0x00000103 => 0.3879934)  …  Dict(0x00000037 => 0.5387884, 0x00000166 => 0.67586136, 0x000000b6 => 0.50291), Dict(0x00000a97 => 0.56313527, 0x0000003c => 0.3634544, 0x00000102 => 0.31194425, 0x00000738 => 0.5609515, 0x00000294 => 0.3725638), Dict(0x000001f9 => 0.6903055, 0x000004b0 => 0.723518), Dict(0x000000b6 => 0.4383699, 0x00000060 => 0.89879465), Dict(0x00000341 => 0.38676968, 0x00000a46 => 0.42377207, 0x000006a3 => 0.4065749, 0x000002d3 => 0.2684107, 0x000000f8 => 0.26224124, 0x00000077 => 0.3272776, 0x0000066e => 0.34812984, 0x00000bbc => 0.36932853), Dict(0x00000037 => 0.26218536, 0x00000463 => 0.40796354, 0x000001ad => 0.2909695, 0x000000d8 => 0.36215267, 0x000009b1 => 0.42634773, 0x000002d5 => 0.3597346, 0x000009b0 => 0.41706774, 0x00000059 => 0.25274485), Dict(0x0000040b => 0.33927795, 0x00000cb9 => 0.5557221, 0x000006a0 => 0.54573447, 0x000001c6 => 0.52748203), Dict(0x0000014d => 0.36407647, 0x0000082e => 0.5690961, 0x000000fe => 0.35886785, 0x00000789 => 0.5174932, 0x000002cc => 0.3833961), Dict(0x0000032f => 0.4749454, 0x00000bec => 0.61925524, 0x00000bed => 0.6252598), Dict(0x0000000d => 0.19462189, 0x00000353 => 0.3761832, 0x00000b66 => 0.36356863, 0x00000cae => 0.3654544, 0x00000c61 => 0.40492746, 0x000000f9 => 0.32582772, 0x00000857 => 0.34660488, 0x000009e6 => 0.34927666, 0x00000017 => 0.20642276)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = VectorDatabase(rand(D.vectors, 1000))\n",
    "fQ = VectorDatabase(rand(fD.vectors, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0f207f2-ed09-4fec-9da6-2ad625c21801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.292747 seconds (1.93 M allocations: 141.872 MiB, 3.65% gc time, 65.59% compilation time)\n",
      "  0.491890 seconds (648.26 k allocations: 34.584 MiB, 67.35% compilation time)\n",
      "  0.239561 seconds (21 allocations: 125.688 KiB)\n",
      "  0.003348 seconds (6 allocations: 125.250 KiB)\n"
     ]
    }
   ],
   "source": [
    "# evaluación exhaustiva\n",
    "brute = ExhaustiveSearch(; db=D.vectors, dist=NormalizedCosineDistance())\n",
    "fbrute = ExhaustiveSearch(; db=fD.vectors, dist=NormalizedCosineDistance())\n",
    "\n",
    "@time searchbatch(brute, Q, 16)\n",
    "@time searchbatch(invfile, Q, 16)\n",
    "@time searchbatch(fbrute, fQ, 16)\n",
    "@time searchbatch(finvfile, fQ, 16)\n",
    "nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad823b-70c0-447d-bfe0-8a920525ca12",
   "metadata": {},
   "source": [
    "## Se volverá a realizar la operación pero con unas pequeñas modificaciones para remover el costo de compilación y el costo de recolección de basura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde30ad5-8c2d-4622-b0c2-994802b64e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.164695 seconds (2.69 k allocations: 280.063 KiB, 7.34% compilation time)\n",
      "  0.484623 seconds (70 allocations: 127.219 KiB)\n",
      "  0.003244 seconds (8 allocations: 125.312 KiB)\n",
      "  0.238250 seconds (22 allocations: 125.719 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GC.enable(false)\n",
    "@time I, _ = searchbatch(invfile, Q, 16)\n",
    "@time G, _ = searchbatch(brute, Q, 16)\n",
    "@time fI, _ = searchbatch(finvfile, fQ, 16)\n",
    "@time fG, _= searchbatch(fbrute, fQ, 16)\n",
    "GC.enable(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc566c35-ebb2-4725-a524-a0737319db22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macrorecall(G, I) = 0.9975\n",
      "macrorecall(fG, fI) = 0.9498125\n",
      "macrorecall(G, fG) = 0.001\n",
      "macrorecall(G, fI) = 0.0009375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0009375"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### La diferencia de costos puede ser importante, por lo que es también necesario saber el impacto con respecto a la calidad perdida\n",
    "@show macrorecall(G, I)\n",
    "@show macrorecall(fG, fI)\n",
    "@show macrorecall(G, fG)\n",
    "@show macrorecall(G, fI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f23a7-1e9c-4091-a63c-84695e325598",
   "metadata": {},
   "source": [
    "# Sobre los resultados\n",
    "Afortunadamente no es tan simple como decir que se destruyo la calidad del resultado,\n",
    "ya que eso implicaría que el modelo sin filtros fuera mejor y eso no es necesariamente\n",
    "cierto ya que los filtros podrían estar removiendo términos _ruidosos_ (errores ortográficos y palabras que al ser tan comunes aportan poco al resultado).\n",
    "Lo que sí se puede afirmar es que los dos conjuntos de resultados son diferentes y esto es importante de tener\n",
    "en cuenta cuando se realizan modificaciones a los modelos.\n",
    "\n",
    "Tradicionalmente, los resultados se miden con el recall, pero el _gold standard_ es construido a partir de\n",
    "intervención humana, que es capaz de decir que documentos son relevantes a una consulta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be69f7-5458-4e1c-b0fa-a14ab2600986",
   "metadata": {},
   "source": [
    "# Actividades\n",
    "\n",
    "- ¿Qué es el recall?\n",
    "- ¿Qué es macro-recall?\n",
    "- Explique el porque de las mejoras en tiempo. Use análisis de algoritmos para este ejercicio.\n",
    "- Implemente un índice invertido que sea capaz de mejorar los tiempos de búsqueda tal y como se muestra en este ejercicio. Para el procesamiento del texto y toquenizado use librerias/paquetes externos.\n",
    "- Reporte mediante un notebook de Jupyter su implementación, use uno de los conjuntos de datos para ejemplificar su uso y medir los desempeños."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
