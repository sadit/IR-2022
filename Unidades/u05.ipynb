{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc189d2-656e-41d4-be84-dcf6ccf9537b",
   "metadata": {},
   "source": [
    "# Modelado semántico del vocabulario\n",
    "Autor: Eric S. Tellez <eric.tellez@infotec.mx>\n",
    "\n",
    "El modelado basado en vocabularios o léxico tiene una gran cantidad de aplicaciones, las cuales son suelen explotar la gran cantidad de grados de libertad en el preprocesamiento, toquenizado y pesado, para ajustar a las tareas deseadas. Sin embargo, dicho modelado tiene problemas basados en la semántica:\n",
    "\n",
    "- si las consultas tienen palabras no conocidas (fuera de vocabulario), no es posible ligar a palabras similares por semántica \n",
    "- las palabras similares en el vocabulario pero no iguales, no puede relacionarse para reducir un vocabulario, o para cualquier otro efecto\n",
    "- las palabras que se escriben igual pero tienen identico significado serán puestas en la misma bolsa\n",
    "\n",
    "De manera más detallada, se tendrán problemas al manejar sinónimos, hipónimos, antónimos, etc. Adicionalmente, los errores serán tratados como palabras diferentes y no ajustados al mejor calce conocido (a menos que exista un corrector ortográfico entre el preprocesamiento).\n",
    "\n",
    "Estos problemas se buscan reducir con representaciones semánticas de los vocabularios y de los textos, esto sería equivalente. El intento sería buscar por conceptos más que por palabras que lo describan.\n",
    "En este punto aparecen los modelos semánticos, los cuales suelen ser representaciones vectoriales densas _embeddings_, de alta dimensión pero mucho menor que las representaciones léxicas (i.e., pasarían a unos pocos cientos en lugar de varias decenas de miles o cientos de miles de coordinadas).\n",
    "\n",
    "# Word embeddings\n",
    "Las representaciones de vectores densos de las palabras se suelen construir utilizando la [_hipótesis distribucional_](https://aclweb.org/aclwiki/Distributional_Hypothesis), que se puede resumir que las palabras con contextos similares (palabras que la rodean) tienen un significado similar. Entonces, para obtener la semántica, es necesario comparar contextos; lo anterior se puede llevar a cabo de diferentes maneras.\n",
    "\n",
    "- Latent semantic analysis (LSA): La matriz $W_{m,n}^*$  con pesos basados en frecuencias es usada para reducir el numero de filas (palabras) mediante _Singular Value Decomposition_ (SVD). La similitud adecuada entre vectores coseno en el espacio de dimensión reducida. Ver [@Dumais2004] para más información.\n",
    "- Latent semantic indexing (LSI): Similar a LSA pero usando esquemas de pesado diferentes, adicionalmente se usa Trucated SVD (TSVD) en lugar de SVD ya que puede seleccionar componentes más importantes. Para más detalles ver [@Hofman1999].\n",
    "- Word2Vec. Se construye una red neuronal con una matriz de pesos del tamaño del vocabulario por una dimensión seleccionada, a partir de los contextos. Revisa los textos de entrenamiento mediante una ventana móvil. De manera especial, se tiene que el centro de la ventana es la palabra objetivo y el contexto esta a su alrededor. Se intenta predecir mediante el contexto la palabra central, y se ajusta mediante propagación hacia atrás. La función de perdida suele ser [softmax](https://en.wikipedia.org/wiki/Softmax_function) o hierachical softmax. Los embeddings no son la capa de salida sino una capa atrás. Para más detalles ver [@MCCD2013].\n",
    "- Glove. Se utiliza una matriz de coocurrencia de términos dentro de una ventana deslizante que analiza una colección de textos. La matriz es altamente dispersa. Se utiliza TSVD para bajar su dimensión. Los detalles se pueden consultar en [@PSM2014]. \n",
    "- FastText. Muy similar a Word2Vec pero añade manejo de palabras fuera de diccionario mediante subwords (similar a q-gramas de carácteres por cada palabra). Ver [@BGJM2017] para mayores detalles.\n",
    "\n",
    "# Language models\n",
    "En los word embeddings, cada palabra del vocabulario tiene asignado un vector denso de manera estática, fruto de su semántica basada en la hipótesis distribucional. Los modelos de lenguaje van más allá, intentando no solo tener en cuenta una palabra para el vector, si no que el vector mismo es dependiente del contexto, por lo cual puede desambiguar de manera natural palabras idénticas (homónimos) usando dicha información contextual. Adicionalmente, capturan información relevante de grandes corpus de texto, aportando muchas veces información de un mundo (i.e., si se entrenan usando la wikipedia, tendrán información relavante de multiples dominios).\n",
    "\n",
    "La punta de lanza de los modelos de lenguaje es el aprendizaje profundo, y más precisamente, con el uso de _transformers_. Uno de los modelos de lenguaje más utilizado es BERT [@VSPU2017]. Estos modelos están fuera del alcance de nuestro curso pero se invitá al interesado a revisar más sobre ellos <https://huggingface.co/>.\n",
    "\n",
    "## Nota\n",
    "Este notebook esta basado en <https://github.com/sadit/SimilaritySearchDemos/blob/main/Glove/Glove.ipynb>. En el notebook original se muestran visualizaciones basadas en todos los vecinos cercanos (se verán más adelante en el curso).\n",
    "\n",
    "# Ejemplo\n",
    "\n",
    "El resto del documento muestra como usar word embeddings Glove para precalculados, para resolver tareas de vecinos cercanos y resolución de analogías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd04785e-aa4e-493c-9aa8-5c54c8e1f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/IR-2022/Unidades`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "using SimilaritySearch, Plots, LinearAlgebra, Embeddings, HypertextLiteral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8bad4e-c31b-4506-8341-47c4130d664a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_index (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_dataset()\n",
    "    emb = load_embeddings(GloVe{:en}, 2)  # you can change with any of the available embeddings in `Embeddings`\n",
    "    for c in eachcol(emb.embeddings)\n",
    "        normalize!(c)\n",
    "    end\n",
    "\n",
    "    db = MatrixDatabase(emb.embeddings)\n",
    "    db, emb.vocab\n",
    "end\n",
    "\n",
    "function create_index()\n",
    "    db, vocab = load_dataset()\n",
    "    dist = NormalizedCosineDistance()\n",
    "    index = SearchGraph(; dist, db, verbose=false)\n",
    "    index!(index)\n",
    "    optimize!(index, MinRecall(0.9))\n",
    "    index, vocab\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb60ff-68c0-4c09-a8dd-70079ba59994",
   "metadata": {},
   "source": [
    "### El índice métrico se crea, además se toma el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9108c0-02a7-4afe-bfa2-7b436f7bd0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16.942226 seconds (11.69 M allocations: 3.329 GiB, 7.05% gc time, 17.64% compilation time)\n"
     ]
    }
   ],
   "source": [
    "@time index, vocab = create_index()\n",
    "vocab2id = Dict(w => i for (i, w) in enumerate(vocab));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e629f24-ecf3-4908-a100-b70559dca89a",
   "metadata": {},
   "source": [
    "### Búsqueda de todos los vecinos cercanos en el vocabulario, observe la conveniencia del uso de un índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43463a8-0c73-4903-a78c-6b2405ddf812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18.988572 seconds (4.40 M allocations: 3.757 GiB, 1.19% gc time, 6.21% compilation time)\n",
      "245.906001 seconds (705.52 k allocations: 131.429 MiB, 0.15% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94238015625"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let k = 32\n",
    "    @time I, _ = allknn(index, k)\n",
    "    # WARNING: Don't run the following line, it takes too much time\n",
    "    @time gI, _ = allknn(ExhaustiveSearch(; db=index.db, dist=index.dist), k)\n",
    "    macrorecall(gI, I)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b68d72-5256-49ad-b22b-e896f58bc29f",
   "metadata": {},
   "source": [
    "### Búsqueda y presentación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e045135e-4189-47cc-9513-72a5000527f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Ejemplos de búsqueda (aleatorios)</h1>"
      ],
      "text/plain": [
       "<h1>Ejemplos de búsqueda (aleatorios)</h1>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.001049 seconds (4 allocations: 1.844 KiB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"zetina\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>zetina</td><td>293562</td><td>-0.0</td></tr><tr><td>2</td><td>lebov</td><td>328833</td><td>0.241</td></tr><tr><td>3</td><td>herdahl</td><td>348355</td><td>0.259</td></tr><tr><td>4</td><td>funches</td><td>317751</td><td>0.265</td></tr><tr><td>5</td><td>dokur</td><td>231975</td><td>0.267</td></tr><tr><td>6</td><td>spiridonova</td><td>373226</td><td>0.267</td></tr><tr><td>7</td><td>nikc</td><td>350644</td><td>0.273</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"zetina\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>zetina</td><td>293562</td><td>-0.0</td></tr><tr><td>2</td><td>lebov</td><td>328833</td><td>0.241</td></tr><tr><td>3</td><td>herdahl</td><td>348355</td><td>0.259</td></tr><tr><td>4</td><td>funches</td><td>317751</td><td>0.265</td></tr><tr><td>5</td><td>dokur</td><td>231975</td><td>0.267</td></tr><tr><td>6</td><td>spiridonova</td><td>373226</td><td>0.267</td></tr><tr><td>7</td><td>nikc</td><td>350644</td><td>0.273</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.001098 seconds (6 allocations: 7.594 KiB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"one-horse\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>one-horse</td><td>394593</td><td>0.0</td></tr><tr><td>2</td><td>four-wheeler</td><td>394105</td><td>0.261</td></tr><tr><td>3</td><td>chauffeur-driven</td><td>399393</td><td>0.262</td></tr><tr><td>4</td><td>whistle-stop</td><td>381400</td><td>0.268</td></tr><tr><td>5</td><td>4-wheeled</td><td>396367</td><td>0.284</td></tr><tr><td>6</td><td>35-seat</td><td>322846</td><td>0.285</td></tr><tr><td>7</td><td>five-seater</td><td>397869</td><td>0.286</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"one-horse\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>one-horse</td><td>394593</td><td>0.0</td></tr><tr><td>2</td><td>four-wheeler</td><td>394105</td><td>0.261</td></tr><tr><td>3</td><td>chauffeur-driven</td><td>399393</td><td>0.262</td></tr><tr><td>4</td><td>whistle-stop</td><td>381400</td><td>0.268</td></tr><tr><td>5</td><td>4-wheeled</td><td>396367</td><td>0.284</td></tr><tr><td>6</td><td>35-seat</td><td>322846</td><td>0.285</td></tr><tr><td>7</td><td>five-seater</td><td>397869</td><td>0.286</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000435 seconds (4 allocations: 1.844 KiB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"kornwestheim\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>kornwestheim</td><td>325549</td><td>0.0</td></tr><tr><td>2</td><td>untertürkheim</td><td>365776</td><td>0.23</td></tr><tr><td>3</td><td>graben-neudorf</td><td>382978</td><td>0.28</td></tr><tr><td>4</td><td>renningen</td><td>389859</td><td>0.28</td></tr><tr><td>5</td><td>neckarelz</td><td>337321</td><td>0.293</td></tr><tr><td>6</td><td>thamshavn</td><td>355234</td><td>0.297</td></tr><tr><td>7</td><td>holzkirchen</td><td>379140</td><td>0.304</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"kornwestheim\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>kornwestheim</td><td>325549</td><td>0.0</td></tr><tr><td>2</td><td>untertürkheim</td><td>365776</td><td>0.23</td></tr><tr><td>3</td><td>graben-neudorf</td><td>382978</td><td>0.28</td></tr><tr><td>4</td><td>renningen</td><td>389859</td><td>0.28</td></tr><tr><td>5</td><td>neckarelz</td><td>337321</td><td>0.293</td></tr><tr><td>6</td><td>thamshavn</td><td>355234</td><td>0.297</td></tr><tr><td>7</td><td>holzkirchen</td><td>379140</td><td>0.304</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function search_and_display(index, vocab, q, res, k, qword)\n",
    "    res = reuse!(res, k)\n",
    "    @time search(index, q, res)\n",
    "\n",
    "    L = []\n",
    "    for (j, (id, d)) in enumerate(res)\n",
    "        push!(L, @htl \"<tr><td>$j</td><td>$(vocab[id])</td><td>$id</td><td>$(round(d, digits=3))</td></tr>\")\n",
    "    end\n",
    "\n",
    "    display(@htl \"\"\"<h2>resultados for \"$qword\"</h2>\n",
    "    <table>\n",
    "    <th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
    "        $L\n",
    "    </table>\n",
    "    \"\"\")\n",
    "end\n",
    "\n",
    "function search_and_display(index, vocab, qid::Integer, res, k=maxlength(res))\n",
    "    search_and_display(index, vocab, index[qid], res, k, vocab[qid])\n",
    "end\n",
    "\n",
    "display(@htl \"<h1>Ejemplos de búsqueda (aleatorios)</h1>\")\n",
    "res = KnnResult(7)\n",
    "for i in 1:3\n",
    "    for qid in rand(1:length(vocab))\n",
    "        search_and_display(index, vocab, qid, res)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b320eacd-d47e-4dbe-a7e9-a240c4535b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000331 seconds (4 allocations: 1.844 KiB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"glove\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>glove</td><td>10923</td><td>0.0</td></tr><tr><td>2</td><td>ball</td><td>1084</td><td>0.369</td></tr><tr><td>3</td><td>gloves</td><td>10808</td><td>0.369</td></tr><tr><td>4</td><td>plate</td><td>4365</td><td>0.379</td></tr><tr><td>5</td><td>pads</td><td>15978</td><td>0.418</td></tr><tr><td>6</td><td>infield</td><td>16731</td><td>0.435</td></tr><tr><td>7</td><td>bounced</td><td>9154</td><td>0.435</td></tr><tr><td>8</td><td>hat</td><td>5626</td><td>0.438</td></tr><tr><td>9</td><td>bat</td><td>4926</td><td>0.439</td></tr><tr><td>10</td><td>fielder</td><td>10241</td><td>0.447</td></tr><tr><td>11</td><td>pitch</td><td>3099</td><td>0.45</td></tr><tr><td>12</td><td>helmet</td><td>11027</td><td>0.464</td></tr><tr><td>13</td><td>catcher</td><td>9065</td><td>0.465</td></tr><tr><td>14</td><td>throw</td><td>3376</td><td>0.466</td></tr><tr><td>15</td><td>nose</td><td>5943</td><td>0.468</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"glove\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>glove</td><td>10923</td><td>0.0</td></tr><tr><td>2</td><td>ball</td><td>1084</td><td>0.369</td></tr><tr><td>3</td><td>gloves</td><td>10808</td><td>0.369</td></tr><tr><td>4</td><td>plate</td><td>4365</td><td>0.379</td></tr><tr><td>5</td><td>pads</td><td>15978</td><td>0.418</td></tr><tr><td>6</td><td>infield</td><td>16731</td><td>0.435</td></tr><tr><td>7</td><td>bounced</td><td>9154</td><td>0.435</td></tr><tr><td>8</td><td>hat</td><td>5626</td><td>0.438</td></tr><tr><td>9</td><td>bat</td><td>4926</td><td>0.439</td></tr><tr><td>10</td><td>fielder</td><td>10241</td><td>0.447</td></tr><tr><td>11</td><td>pitch</td><td>3099</td><td>0.45</td></tr><tr><td>12</td><td>helmet</td><td>11027</td><td>0.464</td></tr><tr><td>13</td><td>catcher</td><td>9065</td><td>0.465</td></tr><tr><td>14</td><td>throw</td><td>3376</td><td>0.466</td></tr><tr><td>15</td><td>nose</td><td>5943</td><td>0.468</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_and_display(index, vocab, vocab2id[\"glove\"], res, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e60967-b159-4e46-a0a1-369efbe70477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63a0d1ea-bdf8-468a-a20d-1bdf0c7a6503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000107 seconds (2 allocations: 416 bytes)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"&lt;father> - &lt;man> + &lt;woman>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>mother</td><td>809</td><td>0.098</td></tr><tr><td>2</td><td>daughter</td><td>1132</td><td>0.132</td></tr><tr><td>3</td><td>wife</td><td>703</td><td>0.147</td></tr><tr><td>4</td><td>husband</td><td>1328</td><td>0.172</td></tr><tr><td>5</td><td>grandmother</td><td>7401</td><td>0.189</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"&lt;father> - &lt;man> + &lt;woman>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>mother</td><td>809</td><td>0.098</td></tr><tr><td>2</td><td>daughter</td><td>1132</td><td>0.132</td></tr><tr><td>3</td><td>wife</td><td>703</td><td>0.147</td></tr><tr><td>4</td><td>husband</td><td>1328</td><td>0.172</td></tr><tr><td>5</td><td>grandmother</td><td>7401</td><td>0.189</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000184 seconds (2 allocations: 1.438 KiB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"&lt;fireman> - &lt;man> + &lt;woman>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>fireman</td><td>27345</td><td>0.156</td></tr><tr><td>2</td><td>firefighter</td><td>15812</td><td>0.303</td></tr><tr><td>3</td><td>paramedic</td><td>33841</td><td>0.393</td></tr><tr><td>4</td><td>rescuer</td><td>44915</td><td>0.439</td></tr><tr><td>5</td><td>passerby</td><td>53776</td><td>0.459</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"&lt;fireman> - &lt;man> + &lt;woman>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>fireman</td><td>27345</td><td>0.156</td></tr><tr><td>2</td><td>firefighter</td><td>15812</td><td>0.303</td></tr><tr><td>3</td><td>paramedic</td><td>33841</td><td>0.393</td></tr><tr><td>4</td><td>rescuer</td><td>44915</td><td>0.439</td></tr><tr><td>5</td><td>passerby</td><td>53776</td><td>0.459</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000061 seconds (2 allocations: 416 bytes)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"&lt;policeman> - &lt;man> + &lt;woman>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>policeman</td><td>6857</td><td>0.144</td></tr><tr><td>2</td><td>wounding</td><td>6118</td><td>0.285</td></tr><tr><td>3</td><td>policemen</td><td>4984</td><td>0.295</td></tr><tr><td>4</td><td>passerby</td><td>53776</td><td>0.306</td></tr><tr><td>5</td><td>wounded</td><td>1392</td><td>0.331</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"&lt;policeman> - &lt;man> + &lt;woman>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>policeman</td><td>6857</td><td>0.144</td></tr><tr><td>2</td><td>wounding</td><td>6118</td><td>0.285</td></tr><tr><td>3</td><td>policemen</td><td>4984</td><td>0.295</td></tr><tr><td>4</td><td>passerby</td><td>53776</td><td>0.306</td></tr><tr><td>5</td><td>wounded</td><td>1392</td><td>0.331</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000279 seconds (2 allocations: 1.438 KiB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"&lt;mississippi> - &lt;usa> + &lt;france>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>france</td><td>388</td><td>0.468</td></tr><tr><td>2</td><td>rhine</td><td>13957</td><td>0.488</td></tr><tr><td>3</td><td>coast</td><td>955</td><td>0.506</td></tr><tr><td>4</td><td>brittany</td><td>15877</td><td>0.506</td></tr><tr><td>5</td><td>southern</td><td>483</td><td>0.506</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"&lt;mississippi> - &lt;usa> + &lt;france>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>france</td><td>388</td><td>0.468</td></tr><tr><td>2</td><td>rhine</td><td>13957</td><td>0.488</td></tr><tr><td>3</td><td>coast</td><td>955</td><td>0.506</td></tr><tr><td>4</td><td>brittany</td><td>15877</td><td>0.506</td></tr><tr><td>5</td><td>southern</td><td>483</td><td>0.506</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function analogy(a, b, c, k)\n",
    "\tv = index[vocab2id[a]] - index[vocab2id[b]] + index[vocab2id[c]]\n",
    "\tnormalize!(v)\n",
    "\tsearch_and_display(index, vocab, v, res, k, \"<$a> - <$b> + <$c>\")\n",
    "end\n",
    "\n",
    "analogy(\"father\", \"man\", \"woman\", 5)\n",
    "analogy(\"fireman\", \"man\", \"woman\", 5)\n",
    "analogy(\"policeman\", \"man\", \"woman\", 5)\n",
    "analogy(\"mississippi\", \"usa\", \"france\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f09a9-75aa-49c5-9e35-4c486defa289",
   "metadata": {},
   "source": [
    "# Actividades \n",
    "- Como se menciono, el uso de redes neuronales se vuelve muy popular para aprender la semántica desde el texto. Comente las razones justificando sus comentarios en términos de aprendizaje y requerimientos computacionales.\n",
    "- Discuta que tipo de métrica se usa y porque.\n",
    "- ¿Cómo se podrían usar los word-embeddings para búsqueda de documentos completos?\n",
    "- Como ajustaría la métrica.\n",
    "- Si usa Julia, revisé el paquete `Embeddings.jl`, `Word2Vec.jl`.\n",
    "- Si usa Python, revisé los paquetes `fastText` y `word2vec`.\n",
    "- Glove <https://nlp.stanford.edu/projects/glove/>\n",
    "- Reproduzca el ejercicio de este notebook (no reproduzca la búsqueda exhaustiva, ya que le tomará muchísimo tiempo), use embeddings para español, cambié los ejemplos. Se sugiere el uso de <https://ingeotec.github.io/regional-spanish-models/> donde encontrará modelos fastText regionalizados del español, pero puede usar otros embeddings.\n",
    "- Reporte su notebook y anote sus soluciones a las preguntas planteadas. Anote sus reflexiones.\n",
    "\n",
    "# Bibliografía\n",
    "- [Dumais2004] Dumais, S. T. (2004). Latent semantic analysis. Annu. Rev. Inf. Sci. Technol., 38(1), 188-230.\n",
    "- [Hofmann1999] Hofmann, T. (1999, August). Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval (pp. 50-57).\n",
    "- [MCCD2013]: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\n",
    "- [PSM2014] Pennington, J., Socher, R., & Manning, C. D. (2014, October). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).\n",
    "- [BGJM2017]: Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with subword information. Transactions of the association for computational linguistics, 5, 135-146.\n",
    "- [VSPU2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
