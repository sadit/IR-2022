{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc189d2-656e-41d4-be84-dcf6ccf9537b",
   "metadata": {},
   "source": [
    "# Representación semántica y búsqueda\n",
    "Autor: Eric S. Tellez <eric.tellez@infotec.mx>\n",
    "\n",
    "## Usando la nube de puntos\n",
    "\n",
    "La representación de nube de puntos puede usarse directamente sin realizar prototipos, sin embargo, es posible que dada su complejidad, dimensión intrinseca, tenga poca utilidad en grandes volumenes de información. Por ejemplo, es posible utilizar la distancia de [Hausdorff](https://en.wikipedia.org/wiki/Hausdorff_distance); que entre dos nubes de puntos (documentos para este caso) esta definida como sigue:\n",
    "\n",
    "$$ H_d(U, V) = \\max \\left\\{ \\max_{u \\in U} \\min_{x \\in V} d(u, x), \\max_{v \\in V} \\min_{x \\in U} d(v, x) \\right\\} $$\n",
    "\n",
    "Donde $\\textsf{nn}$ encuentra el vecino cercano del primer argumento en el segundo. Podemos ver la intuición de esta distancia, analizando las partes de la expresión. \n",
    "La diferencia entre ambos conjuntos se representa con el máximo de las distancias mínimas. Note que esta parte es idéntica a resolver una búsqueda de vecinos cercanos. La parte más externa de la expresión se repite para los dos conjuntos para preservar la simétria de la función.\n",
    "\n",
    "De la misma forma, es posible obtener variaciones de interés como considerar todas las distancias cercanas en lugar de solo las máximas, lo cual puede reducir formas caprichosas de _outlayers_.\n",
    "\n",
    "$$ H^+_d(U, V) = \\max \\left\\{ \\sum_{u \\in U} \\min_{x \\in V} d(u, x), \\sum_{v \\in V} \\min_{x \\in U} d(v, x) \\right\\}$$\n",
    "\n",
    "La información local de una representación de bolsa de palabras también puede ser aprovechada, por ejemplo, añadiendo información de $\\textsf{TF}$\n",
    "\n",
    "$$ H^\\textsf{TF}_d(U, V) = \\max \\left\\{ \\sum_{u \\in U} {\\textsf{TF}(u, U) \\min_{x \\in V} d(u, x)}, \\sum_{v \\in V} {\\textsf{TF}(v, V) \\min_{x \\in U} d(v, x)} \\right\\} $$\n",
    "\n",
    "# Notas adicionales\n",
    "La versión de $H_d$ que usa la probabilidad de término $\\textsf{pt}$ es similar a la aproximación 2 listada en [@KSKW2015]. Los prototipos son similares a la aproximación 1 del mismo artículo. En el artículo se presenta la función de distancia _word mover's distance_ (WMD) que es una adecuación de la Earths mover's distance (EMD) [PW2009]. EMD es la solución óptima a un problema de transportación, la WMD es la adaptación teniendo en cuenta embeddings de palabras.\n",
    "\n",
    "# Ejemplo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd04785e-aa4e-493c-9aa8-5c54c8e1f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/IR-2022/Unidades`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "!isfile(\"Manifest.toml\") && Pkg.add([\n",
    "    PackageSpec(name=\"SimilaritySearch\", version=\"0.9\"),\n",
    "    PackageSpec(name=\"TextSearch\", version=\"0.13\"),\n",
    "    PackageSpec(name=\"Plots\"),\n",
    "    PackageSpec(name=\"KNearestCenters\"),\n",
    "    PackageSpec(name=\"HypertextLiteral\"),\n",
    "    PackageSpec(name=\"CSV\"),\n",
    "    PackageSpec(name=\"DataFrames\"),\n",
    "    PackageSpec(name=\"Word2Vec\"),\n",
    "    PackageSpec(name=\"Embeddings\"),\n",
    "])\n",
    "\n",
    "using SimilaritySearch, TextSearch, Plots, Word2Vec, KNearestCenters, LinearAlgebra, HypertextLiteral, CSV, DataFrames, Base64, Random\n",
    "using Embeddings: load_embeddings\n",
    "using Downloads: download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9f7f49-5c32-41ea-95f5-b8f1670c2842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m      Status\u001b[22m\u001b[39m `~/IR-2022/Unidades/Project.toml`\n",
      " \u001b[90m [944b1d66] \u001b[39mCodecZlib v0.7.0\n",
      " \u001b[90m [ac1192a8] \u001b[39mHypertextLiteral v0.9.4\n",
      " \u001b[90m [b20bd276] \u001b[39mInvertedFiles v0.4.2 `../../Research/InvertedFiles.jl`\n",
      " \u001b[90m [682c06a0] \u001b[39mJSON v0.21.3\n",
      " \u001b[90m [4dca28ae] \u001b[39mKNearestCenters v0.7.1\n",
      " \u001b[90m [8ef0a80b] \u001b[39mLanguages v0.4.3\n",
      " \u001b[90m [eb30cadb] \u001b[39mMLDatasets v0.7.3\n",
      " \u001b[90m [053f045d] \u001b[39mSimilaritySearch v0.9.4 `../../Research/SimilaritySearch.jl`\n",
      " \u001b[90m [fb8f903a] \u001b[39mSnowball v0.1.0\n",
      " \u001b[90m [2913bbd2] \u001b[39mStatsBase v0.33.18\n",
      " \u001b[90m [7f6f6c8a] \u001b[39mTextSearch v0.12.7 `../../Research/TextSearch.jl`\n",
      " \u001b[90m [c64b6f0f] \u001b[39mWord2Vec v0.5.3\n"
     ]
    }
   ],
   "source": [
    "]status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9bcb8-60f5-42d5-adb3-dca1f1b480b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fe195c4-8f5a-4b8f-bde9-8be620397ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word2vec_embeddings (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function word2vec_embeddings(textconfig, corpus, embfile; dim=32, w2vtmp=tempfile(pwd()))\n",
    "    isfile(embfile) && return\n",
    "    tcorpus = tokenize_corpus(textconfig, corpus)\n",
    "    voc = Vocabulary(textconfig, tcorpus)\n",
    "    fvoc = filter_tokens(voc) do t\n",
    "        5 <= t.ndocs <= 2000\n",
    "    end\n",
    "    valid = Set(fvoc.token)\n",
    "\n",
    "    open(w2vtmp, \"w\") do f\n",
    "        for tokens in tcorpus\n",
    "            tokens = filter!(t -> t in valid, tokens)\n",
    "            if length(tokens) > 7 # minimum number of tokens in a message\n",
    "                println(f, join(tokens, ' '))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    word2vec(w2vtmp, embfile, size=dim, iter=15, threads=Threads.nthreads()-1)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d4602c-c9d6-4a52-a451-1d482fa5413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "textconfig = TextConfig(; nlist=[1], qlist=[], slist=[], lc=true,\n",
    "    group_usr=true, group_url=true, group_num=true, del_diac=true, del_punc=true)\n",
    "\n",
    "filename = \"../data/spanish-news-in-twitter.csv\"\n",
    "embfile = filename * \".w2v.vec\"\n",
    "@time D = CSV.read(filename, DataFrame; delim=',', missingstring=\"XXXX\")\n",
    "@time D[:, :text] = String.(base64decode.(D.text))\n",
    "rm(embfile)\n",
    "word2vec_embeddings(textconfig, D.text, embfile; dim=64, w2vtmp=joinpath(pwd(), \"w2v.tmp\"))\n",
    "E = load_embeddings(FastText_Text, embfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ccc0b-aacb-4bc8-b36e-7effdb3647dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = let corpus=D.text\n",
    "    voc = Vocabulary(textconfig, corpus; thesaurus=E.vocab)\n",
    "    model = VectorModel(BinaryGlobalWeighting(), TpWeighting(), voc)\n",
    "    (; textconfig, model, voc)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b2661-122c-412b-afe8-ac0ec4c4b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud(text, E, T) = vectorize(T.model, T.textconfig, text; normalize=false)\n",
    "cloud_corpus(corpus, E, T) = vectorize_corpus(T.model, T.textconfig, corpus; normalize=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fcb23f-9564-4910-9a4f-1f5d4a582d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct AWMD <: SemiMetric\n",
    "    X::MatrixDatabase\n",
    "    results::Vector{KnnResult}\n",
    "    dcos::NormalizedCosineDistance\n",
    "end\n",
    "\n",
    "AWMD(X) = AWMD(X, [KnnResult(1) for _ in 1:Threads.nthreads()], NormalizedCosineDistance())\n",
    "\n",
    "const PI_2 = convert(Float32, pi / 2)\n",
    "\n",
    "function nn_(wmd, res, dcos, u, V)\n",
    "    res = reuse!(res)\n",
    "\n",
    "    for vid in keys(V)\n",
    "        v = wmd.X[vid]\n",
    "        push!(res, vid, evaluate(dcos, u, v))\n",
    "    end\n",
    "    \n",
    "    minimum(res)\n",
    "end\n",
    "\n",
    "function onesidewmd(wmd, res, dcos, U, V)\n",
    "    d = zero(Float32)\n",
    "    @fastmath for (uid, w) in U\n",
    "        if !haskey(V, uid) # evaluates to zero\n",
    "            d = max(d, w * nn_(wmd, res, dcos, wmd.X[uid], V))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    d\n",
    "end\n",
    "\n",
    "function SimilaritySearch.evaluate(wmd::AWMD, U::Dict, V::Dict)\n",
    "    length(U) == 1 && haskey(U, 0) && return PI_2\n",
    "    length(V) == 1 && haskey(V, 0) && return PI_2\n",
    "    res = wmd.results[Threads.threadid()]\n",
    "    dcos = wmd.dcos\n",
    "\n",
    "    max(onesidewmd(wmd, res, dcos, U, V), onesidewmd(wmd, res, dcos, V, U))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8bad4e-c31b-4506-8341-47c4130d664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = VectorDatabase(cloud_corpus(D.text, E, T))\n",
    "for c in eachcol(E.embeddings)\n",
    "    normalize!(c)\n",
    "end\n",
    "\n",
    "dist = AWMD(MatrixDatabase(E.embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1dc14b-02cb-45ab-8466-5783251ef877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#=recall = 0.8\n",
    "index = SearchGraph(; dist, db, verbose=true)\n",
    "#index!(index; callbacks=SearchGraphCallbacks(MinRecall(recall)))\n",
    "index!(index)\n",
    "optimize!(index, MinRecall(recall))\n",
    "=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fded63-d4ca-42f6-b69e-b0a044c4396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ParallelExhaustiveSearch(; db, dist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eaad27-034e-437a-92e1-0d7a54500823",
   "metadata": {},
   "outputs": [],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c515d9-d81d-4d91-9087-df312a117242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@time res, _ = search(ex, db[111], KnnResult(10))\n",
    "#=@time res, _ = search(ex, cloud(\"covid en México\", E, T), KnnResult(10))\n",
    "\n",
    "for (id_, dist_) in res\n",
    "    display(@htl \"\"\"<div style=\"padding: 0.5em;\">\n",
    "        <span style=\"width: 25%; margin: 0.25em;\"> $(id_ => round(dist_; digits=3))</span>\n",
    "        <span style=\"background-color: rgb(120, 60, 60); margin: 0.2em;\">$(D.text[id_])</span>\n",
    "    </div>\"\"\")\n",
    "end\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b68d72-5256-49ad-b22b-e896f58bc29f",
   "metadata": {},
   "source": [
    "### Búsqueda y presentación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045135e-4189-47cc-9513-72a5000527f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "function search_and_display(index, qtext, k, D, E, T)\n",
    "    res = KnnResult(k)\n",
    "    q = cloud(qtext, E, T)\n",
    "    @time search(index, q, res)\n",
    "    \n",
    "    L = []\n",
    "    for (j, (id, d)) in enumerate(res)\n",
    "        push!(L, @htl \"<tr><td>$j</td><td>$id</td><td>$(round(d, digits=3))</td> <td>$(D.screen_name[id])</td><td> $(D.text[id])</td> </tr>\")\n",
    "    end\n",
    "\n",
    "    display(@htl \"\"\"<h2>resultados para \"$qtext\"</h2>\n",
    "    <table>\n",
    "    <th>  <td>id</td> <td>dist</td> <td>user</td> </td>message<td> </th>\n",
    "        $L\n",
    "    </table>\n",
    "    \"\"\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1123924-05b2-4799-b2f7-18b3d2c46f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(@htl \"<h1>Ejemplos de búsqueda</h1>\")\n",
    "search_and_display(index, \"el gobierno de andres manuel lopez\", 7, D, E, T)\n",
    "search_and_display(index, \"trafico de drogas\", 7, D, E, T)\n",
    "search_and_display(index, \"covid corona virus\", 7, D, E, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4028733-5dd7-46e0-addb-5f7959766695",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(@htl \"<h1>Ejemplos de búsqueda (mensajes aleatorios)</h1>\")\n",
    "\n",
    "for i in 1:3\n",
    "    for qid in rand(1:length(D.text))\n",
    "        search_and_display(index, D.text[qid], 7, D, E, T)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f09a9-75aa-49c5-9e35-4c486defa289",
   "metadata": {},
   "source": [
    "# Actividades\n",
    "- Reproduzca el ejercicio de este notebook, use embeddings para español, cambié los ejemplos. Se sugiere el uso de <https://ingeotec.github.io/regional-spanish-models/> donde encontrará modelos fastText regionalizados del español, pero puede usar otros embeddings.\n",
    "- ¿Qué piensa de las diferencias de tamaño entre los documentos y las consultas? esto como afecta a la representación semántica.\n",
    "- ¿Cuál sería el símil de bigramas y trigramas para este esquema de representación semántica? Implementelo.\n",
    "- Defina $H^{pt}_d$, donde $pt$ es la probabilidad de ocurrencia de término.- Defina $H^{pt}_d$, donde $pt$ es la probabilidad de ocurrencia de término.\n",
    "- Implemente $H^\\textsf{TF}_d$ y $H^\\textsf{pt}_d$ de manera secuencial y con un índice métrico. Si usa Julia considere `SimilaritySearch.jl` y si usa Python considere `faiss`.\n",
    "- Reporte su notebook y anote sus soluciones a las preguntas planteadas. El reporte deberá contener un ensayo de [@KSKW2015] como introducción. Reporte los resultados de sus implementaciones, compare contra las alternativas presentadas en este reporte. Discuta sus resultados. Finalice el reporte  con reflexiones sobre el uso de nubes de puntos en lugar de bolsas de palabras tradicionales. Anoté sus conclusiones.\n",
    "\n",
    "# Bibliografía\n",
    "- [KSKW2015] Kusner, M., Sun, Y., Kolkin, N., & Weinberger, K. (2015, June). From word embeddings to document distances. In International conference on machine learning (pp. 957-966). PMLR.\n",
    "- [PW2009] Pele, O., & Werman, M. (2009, September). Fast and robust earth mover's distances. In 2009 IEEE 12th international conference on computer vision (pp. 460-467). IEEE.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
